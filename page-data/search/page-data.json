{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"이 글은 우테코 달록팀 크루 나인이 작성했습니다. UI가 비슷하면 재사용 초반에 개발을 진행할 때 은 대부분 UI였습니다. \"UI가 비슷하면 분명 재사용될 것이니 이 부분들을 묶어서 컴포넌트화하자!\" 다음은 달록의 카테고리 목록을 조회하는 페이지입니다.  BE 공식일정 카테고리 알록달록 팀 회의 카테고리 두 카테고리가 매우 흡사하게 생겼죠? 비슷하게 생겼…","fields":{"slug":"/seperate-components/"},"frontmatter":{"date":"August 01, 2022","title":"컴포넌트 분리 기준","tags":["react"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [나인](https://github.com/jhy979)이 작성했습니다.\n\n## UI가 비슷하면 재사용\n\n> 초반에 개발을 진행할 때 `재사용의 기준, 컴포넌트의 분리 기준`은 대부분 UI였습니다.\n\n\"UI가 비슷하면 분명 재사용될 것이니 이 부분들을 묶어서 컴포넌트화하자!\"\n\n다음은 달록의 카테고리 목록을 조회하는 페이지입니다.\n\n![](https://velog.velcdn.com/images/jhy979/post/efeb82b3-fc46-4acc-b9d3-2df795c0fe51/image.png)\n\n- BE 공식일정 카테고리\n- 알록달록 팀 회의 카테고리\n\n두 카테고리가 매우 흡사하게 생겼죠? 비슷하게 생겼으니 하나의 컴포넌트로 취급하여 사용했습니다. (CategoryItem)\n\n```tsx\nfunction CategoryItem({ category, subscriptionId }: CategoryItemProps) {\n  // ... 생략\n  \n  // ⚠️구독을 위한 react query\n  const { mutate: postSubscription } = useMutation<\n    AxiosResponse<Pick<SubscriptionType, 'color'>>,\n    AxiosError,\n    Pick<SubscriptionType, 'color'>,\n    unknown\n  >(() => subscriptionApi.post(accessToken, category.id, body), {\n    onSuccess: () => {\n      queryClient.invalidateQueries(CACHE_KEY.SUBSCRIPTIONS);\n    },\n  });\n\n  // ⚠️구독 해제를 위한 react query\n  const { mutate: deleteSubscription } = useMutation(\n    () => subscriptionApi.delete(accessToken, subscriptionId),\n    {\n      onSuccess: () => {\n        queryClient.invalidateQueries(CACHE_KEY.SUBSCRIPTIONS);\n      },\n    }\n  );\n\n  // ⚠️구독 해제 로직 \n  const unsubscribe = () => {\n    if (window.confirm(CONFIRM_MESSAGE.UNSUBSCRIBE)) {\n      deleteSubscription();\n    }\n  };\n\n  // ⚠️구독 버튼을 눌렀을 때 구독 여부에 따라 수행해야할 로직 변경\n  const handleClickSubscribeButton = () => {\n    subscriptionId > 0 ? unsubscribe() : postSubscription(body);\n  };\n\n  return (\n    <div css={categoryItem(theme)}>\n      <span css={item}>{category.createdAt.split('T')[0]}</span>\n      <span css={item}>{category.name}</span>\n      <div css={item}>\n        // ⚠️구독 여부에 따른 버튼 스타일링 변경\n        <SubscribeButton\n          isSubscribing={subscriptionId > 0}\n          handleClickSubscribeButton={handleClickSubscribeButton}\n        ></SubscribeButton>\n      </div>\n    </div>\n  );\n}\n\nexport default CategoryItem;\n\n```\n구독 여부에 따라 버튼 스타일링만 다르게 하였고 실제로 큰 문제없이 의도한대로 렌더링되었습니다.\n\n---\n\n## 하지만.. 문제 발생\n\n> 문제는 API를 연동했을 때 발생했습니다. (부끄럽게도 컴포넌트에 많은 고민과 설계 없이 구현한 대가를 치룬 것이죠.)\n\n![](https://velog.velcdn.com/images/jhy979/post/ebd07538-e6d0-48cf-a43c-e04562ba3b65/image.png)\n\nBE 공식일정은 이미 구독중인 상태여서 버튼을 누르면 `구독해제` api가 실행되어야합니다.\n\n![](https://velog.velcdn.com/images/jhy979/post/236b2064-2dba-4e75-8130-f4074fe20732/image.png)\n알록달록 팀 회의는 버튼을 누르면 `구독` api가 실행되어야합니다.\n\n😢 문제는 바로 두 카테고리의 `데이터 스키마가 다르다는 점`이였습니다.\n\n\n- `구독 api`는 구독을 위한 `카테고리 id`가 필요합니다. (애초에 구독을 안했기 때문에 구독 id가 존재할 수 없습니다.)\n\n- `구독 해제 api`에서는 카테고리 id가 아닌 구독으로 새롭게 발급된 `구독 id`가 필요합니다.\n\n구독 여부와 관계없이 하나의 컴포넌트로(CategoryItem) 묶었을 때에는 위 2가지 케이스로 인해 `구독 id`에서 문제가 발생합니다. \n\n하나의 컴포넌트로 묶었기 때문에 구독id가 존재하든 존재하지 않든 subscriptionId라는 필드가 반드시 존재해야합니다.\n\n😱 요리조리 머리를 굴려 `구독하지 않았을 때에는 구독 id를 -1로 할당했습니다.`\n(아마 위의 코드를 읽으시면서 subscriptionId > 0인 경우 구독 중이라고 판단하는 로직을 어색하게 느끼셨을 겁니다.)\n\n물론 실제 DB에서 구독 id에 -1이 할당되는 경우는 없기 때문에 큰 문제는 아닐 수 있습니다.\n\n---\n## 새로운 컴포넌트의 분리기준: 데이터 스키마, 모델\n\n하지만 태초에 \"이런 컴포넌트 설계가 맞을까?\" 라는 생각이 들었고 팀 회의를 통해 컴포넌트의 분리 기준을 `✨데이터 스키마와 모델✨`에 초점을 맞추는 방향으로 바꾸었습니다.\n\n> 즉, 구독 중인 카테고리와 구독하지 않은 카테고리는 구독id의 존재 여부가 다르기 때문에 데이터 스키마가 다르다고 말할 수 있겠습니다.\n\n따라서, 구독 중인 카테고리 컴포넌트와 구독하지 않은 카테고리 컴포넌트 2가지 컴포넌트로 분리하였습니다.\n\n---\n\n실제 코드를 볼까요?\n\n### 👇 구독하지 않은 컴포넌트 (UnsubscribedCategoryItem)\n\n![](https://velog.velcdn.com/images/jhy979/post/236b2064-2dba-4e75-8130-f4074fe20732/image.png)\n\n- 구독 id가 없습니다.\n- 구독 react query가 있습니다.\n\n```tsx\n// ⚠️subscriptionId를 아예 받지 않음\nfunction UnsubscribedCategoryItem({ category }: UnsubscribedCategoryItemProps) {\n  // ... 생략\n\n  // ⚠️구독을 위한 react query\n  const { mutate } = useMutation<\n    AxiosResponse<Pick<SubscriptionType, 'color'>>,\n    AxiosError,\n    Pick<SubscriptionType, 'color'>,\n    unknown\n  >(() => subscriptionApi.post(accessToken, category.id, body), {\n    onSuccess: () => {\n      queryClient.invalidateQueries(CACHE_KEY.SUBSCRIPTIONS);\n    },\n  });\n\n  // ⚠️오직 한 가지 일만 담당하는 핸들러 함수\n  const handleClickSubscribeButton = () => {\n    mutate(body);\n  };\n\n  return (\n    <div css={categoryItem}>\n      <span css={item}>{category.createdAt.split('T')[0]}</span>\n      <span css={item}>{category.name}</span>\n      <div css={item}>\n        <Button cssProp={subscribeButton(theme)} onClick={handleClickSubscribeButton}>\n          구독\n        </Button>\n      </div>\n    </div>\n  );\n}\n```\n\n---\n### 👇 구독중인 컴포넌트 (SubscribedCategoryItem)\n\n![](https://velog.velcdn.com/images/jhy979/post/ebd07538-e6d0-48cf-a43c-e04562ba3b65/image.png)\n\n- 구독 id가 있습니다.\n- 구독 해제 react query가 있습니다.\n\n```tsx\nfunction SubscribedCategoryItem({ category, subscriptionId }: SubscribedCategoryItemProps) {\n  // ... 생략\n  \n  // ⚠️구독 해제를 위한 react query\n  const { mutate } = useMutation(() => subscriptionApi.delete(accessToken, subscriptionId), {\n    onSuccess: () => {\n      queryClient.invalidateQueries(CACHE_KEY.SUBSCRIPTIONS);\n    },\n  });\n\n  // ⚠️오직 한 가지 일만하는 핸들러 함수\n  const handleClickUnsubscribeButton = () => {\n    if (window.confirm(CONFIRM_MESSAGE.UNSUBSCRIBE)) {\n      mutate();\n    }\n  };\n\n  return (\n    <div css={categoryItem}>\n      <span css={item}>{category.createdAt.split('T')[0]}</span>\n      <span css={item}>{category.name}</span>\n      <div css={item}>\n        <Button cssProp={unsubscribeButton(theme)} onClick={handleClickUnsubscribeButton}>\n          구독중\n        </Button>\n      </div>\n    </div>\n  );\n}\n```\n\n---\n### 마무리\n\n> 어떤가요?\n\n💪 subscriptionId가 필요하지 않은 경우에는 애초에 props로 들어가지 않기 때문에 `subscriotionId가 -1이 되는 경우가 없습니다.`\n\n또한 react query와 관련된 로직들은 어떠한가요? \n\n💪 관심사 분리가 적절히 이루어져 각 컴포넌트에서 알맞게 호출되고 있습니다.\n\n\n> UI를 기준으로 컴포넌트를 나누는 것도 좋지만 먼저 데이터 스키마에 따라 컴포넌트를 분리하는 것을 먼저 고려해보면 더 좋을 것이라고 이번 리팩토링을 통해 느끼게 되었습니다.\n\n"},{"excerpt":"이 글은 우테코 달록팀 크루 나인이 작성했습니다. React-Query의 캐싱개념은 stale과 cacheTime을 통해 이루어집니다. Stale 사전적 의미로 '신선하지 않은' 입니다. react query는 기본적으로 캐싱된 데이터를 stale하다고 생각합니다. react query에서는 stale time의 default이 0입니다. (즉, 캐싱이 …","fields":{"slug":"/query-invalidation/"},"frontmatter":{"date":"August 01, 2022","title":"React-Query에서의 데이터 최신화 (Query Invalidation)","tags":["react","react-query"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [나인](https://github.com/jhy979)이 작성했습니다.\n\nReact-Query의 캐싱개념은 stale과 cacheTime을 통해 이루어집니다.\n\n## Stale\n> 사전적 의미로 '신선하지 않은' 입니다. react query는 기본적으로 캐싱된 데이터를 stale하다고 생각합니다.\n\nreact query에서는 stale time의 default이 0입니다. (즉, 캐싱이 되지 않는다고 볼 수 있겠죠?)\n\n### staleTime\n> 데이터가 fresh한 상태에서 stale한 상태로 변하는 시간입니다.\n\n- fresh 상태일때는 쿼리 인스턴스가 새롭게 mount 되어도 fetch가 일어나지 않습니다. \n\n- 데이터가 fetch 되고 나서 staleTime이 지나지 않았다면 unmount 후 mount 되어도 fetch가 일어나지 않습니다.\n\n### cacheTime\n> 데이터가 inactive 상태일 때 캐싱된 상태로 남아있는 시간입니다.\n\n- 쿼리 인스턴스가 unmount 되면 데이터는 inactive 상태로 변경되며, 캐시는 cacheTime만큼 유지됩니다.\n\n- cacheTime은 staleTime과 관계없이, 무조건 `inactive된 시점`을 기준으로 캐싱을 결정합니다.\n\n---\n## 달록에서의 데이터 최신화\n\n> 달록은 일정, 카테고리, 구독 등의 작업으로 인해 데이터의 변화가 잦은 어플리케이션입니다. 따라서 저희 팀은 staleTime을 지정해주지 않았습니다.\n\n그렇다면 stale한 데이터는 늘 최신화가 필요할 것입니다.\n\n그래서 저는 매번 새로운 데이터가 필요할 때마다 `useQuery의 refetch`를 강제적으로 실행시켜주는 방식을 생각했었습니다.\n\n예를 들면 일정을 추가(post)한 후에 일정을 다시 조회(get)하는 경우가 있을겁니다.\n\n![](https://velog.velcdn.com/images/jhy979/post/287f66a2-8b7e-49c8-b623-c05a491db600/image.png)\n\n```tsx\n// refetch 함수를 부모로 부터 주입 받아야 합니다.\nfunction ScheduleAddModal({ closeModal, refetch }: ScheduleAddModalProps) {\n  const onSuccessPostSchedule = () => {\n    refetch();\n  };\n\n```\n\n😢 하지만 이 방식이 우아하지는 않더라구요. \n\n만약 부모와 자식 간의 props전달이 아니라 조부모와 자식 간의 props 전달이라면 어떨까요? 벌써 머리가 아파옵니다. (props hell)\n\n\n## Query Invalidation 도입\n🤔 다른 방법이 있을텐데.. 어떤 방법이 있을까? 고민하며 공식문서를 읽던 도중 `Query Invalidation`을 발견하게 되었습니다.\n\n```tsx\n// Invalidate every query in the cache\nqueryClient.invalidateQueries()\n// Invalidate every query with a key that starts with `todos`\nqueryClient.invalidateQueries(['todos'])\n```\n> The QueryClient has an invalidateQueries method that lets you intelligently mark queries as stale and potentially refetch them too!\n\n❗ 캐싱키로 관련된 stale 쿼리들을 체크하고 refetch할 수 있는 메서드가 존재했습니다.\n\n```tsx\nfunction ScheduleAddModal({ closeModal }: ScheduleAddModalProps) {\n  const queryClient = useQueryClient();\n\n  const onSuccessPostSchedule = () => {\n    // 일정 post 성공 시 데이터 최신화를 invalidateQueries메서드를 통해 수행합니다.\n    queryClient.invalidateQueries(CACHE_KEY.SCHEDULES);\n  };\n```\n💪 이로써 useQuery의 refetch함수를 넘겨줄 필요없이 어디서든 캐싱키로 fresh한 데이터를 보장할 수 있게 되었습니다.\n\n\n\n#### 참고자료\nhttps://tanstack.com/query/v4/docs/guides/query-invalidation"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. @SpringBootTest  애노테이션을 사용하면 우리 애플리케이션에서 사용하고 있는 모든 빈을 등록한 뒤 간편하게 테스트를 진행한다. 하지만 모든 빈을 등록하기 때문에 아래와 같은 단점을 가질 수 있다. 모든 빈들을 등록하기 때문에 비교적 오랜 시간이 걸린다. 모든 빈들을 등록하기 때문에 의존성을 …","fields":{"slug":"/integration-test-slice-test/"},"frontmatter":{"date":"August 01, 2022","title":"통합 테스트와 슬라이스 테스트","tags":["통합 테스트","슬라이스 테스트"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## @SpringBootTest\n\n`@SpringBootTest` 애노테이션을 사용하면 우리 애플리케이션에서 사용하고 있는 모든 빈을 등록한 뒤 간편하게 테스트를 진행한다. 하지만 모든 빈을 등록하기 때문에 아래와 같은 단점을 가질 수 있다.\n\n- 모든 빈들을 등록하기 때문에 비교적 오랜 시간이 걸린다.\n- 모든 빈들을 등록하기 때문에 의존성을 고려하지 않고 테스트를 진행할 수 있다. 즉 테스트 하고자 하는 객체의 의존성을 무시한채 테스트하게 된다.\n- 도메인 혹은 영속 계층의 경우 application 계층, presentation 계층에 대해 의존하지 않기 때문에 필요 없는 리소스에 대한 소모가 늘어난다.\n\n이러한 `@SpringBootTest`는 모든 빈을 등록한 채 테스트를 진행하는 `통합 테스트`에 적합한 애노테이션이다.\n\n아래는 실제 프로젝트에 작성한 테스트 중 일부를 가져온 것이다. \n\n```java\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\n@Import(TestConfig.class)\nclass AcceptanceTest {\n\n    @LocalServerPort\n    private int port;\n\n    @Autowired\n    private DatabaseCleaner databaseCleaner;\n\n    @BeforeEach\n    void setUp() {\n        RestAssured.port = port;\n        databaseCleaner.execute();\n    }\n}\n```\n\n```java\n@DisplayName(\"구독 관련 기능\")\n@Import(TestConfig.class)\npublic class SubscriptionAcceptanceTest extends AcceptanceTest {\n\n    @DisplayName(\"인증된 회원이 카테고리를 구독하면 201을 반환한다.\")\n    @Test\n    void 인증된_회원이_카테고리를_구독하면_201을_반환한다() {\n        // given\n        String accessToken = 자체_토큰을_생성하고_토큰을_반환한다(GOOGLE_PROVIDER, 인증_코드);\n        CategoryResponse 공통_일정 = 새로운_카테고리를_등록한다(accessToken, 공통_일정_생성_요청);\n\n        // when\n        ExtractableResponse<Response> response = RestAssured.given().log().all()\n                .auth().oauth2(accessToken)\n                .contentType(MediaType.APPLICATION_JSON_VALUE)\n                .body(빨간색_구독_생성_요청)\n                .when().post(\"/api/members/me/categories/{categoryId}/subscriptions\", 공통_일정.getId())\n                .then().log().all()\n                .statusCode(HttpStatus.CREATED.value())\n                .extract();\n\n        // then\n        상태코드_201이_반환된다(response);\n    }\n    ...\n}\n```\n\n인수 테스트의 경우 사용자의 시나리오에 맞춰 수행하는 테스트이기 때문에 실제 운영 환경과 유사하게 테스트를 진행해야 한다. 그렇기 때문에 슬라이스 테스트를 진행하는 것 보다 모든 빈들을 등록하여 시나리오를 적절히 수행하는지 집중해야 하기 때문에 `@SpringBootTest`를 활용한 통합 테스트를 진행해야 한다.\n\n## 슬라이스 테스트\n\n앞서 언급한 것 처럼 특정 계층은 다른 계층에 의존하지 않기 때문에 필요한 빈들만 주입받아 독립적으로 테스트할 수 있다. 이렇게 계층별로 필요한 빈들만 주입받기 위해서 Spring은 슬라이스 테스트관련 애노테이션을 제공한다.\n\n슬라이스 테스트란? 레이어를 독립적으로 테스트하기 위해 필요한 빈들만 주입 받아 테스트를 진행하는 것을 의미한다.. 슬라이스 테스트를 적절히 활용하면 모든 빈들을 ApplicationContext에 등록하지 않기 때문에 보다 더 호율적으로 테스트가 가능하다.\n\n### @WebMvcTest\n\n`@WebMvcTest`는 웹 계층 테스트를 위해 필요한 빈들이 주입된다. 주입 되는 빈의 항목은 아래와 같다.\n\n- `@Controller`\n- `@ControllerAdvice`\n- `@JsonComponent`\n- `Converter`\n- `Filter`\n- `WebMvcConfigurer`\n- `HandlerMethodArgumentResolver`\n- `MockMvc`\n- 등\n\n아래는 실제 프로젝트에 작성한 테스트 중 일부를 가져온 것이다.\n\n```java\n@AutoConfigureRestDocs\n@WebMvcTest(SubscriptionController.class)\nclass SubscriptionControllerTest {\n\n    private static final String AUTHORIZATION_HEADER_NAME = \"Authorization\";\n    private static final String AUTHORIZATION_HEADER_VALUE = \"Bearer aaaaa.bbbbb.ccccc\";\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @Autowired\n    private ObjectMapper objectMapper;\n\n    @MockBean\n    private AuthService authService;\n\n    @MockBean\n    private SubscriptionService subscriptionService;\n\n    @DisplayName(\"회원과 카테고리 정보를 기반으로 구독한다.\")\n    @Test\n    void 회원과_카테고리_정보를_기반으로_구독한다() throws Exception {\n        // given\n        CategoryResponse 공통_일정_응답 = 공통_일정_응답(관리자_응답);\n        SubscriptionResponse 빨간색_구독_응답 = 빨간색_구독_응답(공통_일정_응답);\n\n        given(authService.extractMemberId(any())).willReturn(매트_응답.getId());\n        given(subscriptionService.save(any(), any(), any())).willReturn(빨간색_구독_응답);\n\n        // when & then\n        mockMvc.perform(post(\"/api/members/me/categories/{categoryId}/subscriptions\", 빨간색_구독_응답.getId())\n                        .header(AUTHORIZATION_HEADER_NAME, AUTHORIZATION_HEADER_VALUE)\n                        .accept(MediaType.APPLICATION_JSON).contentType(MediaType.APPLICATION_JSON)\n                        .content(objectMapper.writeValueAsString(빨간색_구독_생성_요청)))\n                .andDo(print())\n                .andDo(document(\"subscription/save\",\n                        preprocessRequest(prettyPrint()),\n                        preprocessResponse(prettyPrint()),\n                        pathParameters(\n                                parameterWithName(\"categoryId\").description(\"카테고리 id\")\n                        ),\n                        requestHeaders(\n                                headerWithName(\"Authorization\").description(\"JWT 토큰\")\n                        ),\n                        requestFields(\n                                fieldWithPath(\"color\").type(JsonFieldType.STRING).description(\"구독 색 정보\")\n                        )))\n                .andExpect(status().isCreated());\n    }\n    ...\n]\n```\n\n컨트롤러 테스트의 경우 실제 동작하는 로직을 활용하는 것 보다 API의 문서화에 집중했기 때문에 웹 계층에 대한 의존성만 추가한 뒤 의존하는 객체는 Mocking을 통해 진행하였다.\n\n### @DataJpaTest\n\nSpring Data JPA를 사용하고 있다면 테스트하기 위해 간단히 `@DataJpaTest`를 활용할 수 있다. `@Entity` 객체, `JpaRepository` 등 JPA 사용에 필요한 빈들을 등록하여 테스트할 때 사용한다. 아래는 실제 `@DataJpaTest`의 코드를 가져온 것이다.\n\n```java\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@BootstrapWith(DataJpaTestContextBootstrapper.class)\n@ExtendWith(SpringExtension.class)\n@OverrideAutoConfiguration(enabled = false)\n@TypeExcludeFilters(DataJpaTypeExcludeFilter.class)\n@Transactional\n@AutoConfigureCache\n@AutoConfigureDataJpa\n@AutoConfigureTestDatabase\n@AutoConfigureTestEntityManager\n@ImportAutoConfiguration\npublic @interface DataJpaTest {\n\t...\n}\n```\n\n주의 깊게 봐야할 애노테이션들이 많다. 몇 가지 예시로 `@AutoConfigureTestDatabase`, `@Transactional`에 대해 간단히 살펴보자.\n\n#### @AutoConfigureTestDatabase\n\n```java\n...\n@AutoConfigureTestDatabase\n...\npublic @interface DataJpaTest {\n\t...\n}\n```\n\n애플리케이션에 정의되어 있거나 자동으로 설정된 DataSoruce를 대신하여 테스트용 DB를 정의할 때 사용된다. 아래 실제 코드를 살펴보자.\n\n```java\n@Target({ ElementType.TYPE, ElementType.METHOD })\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@ImportAutoConfiguration\n@PropertyMapping(\"spring.test.database\")\npublic @interface AutoConfigureTestDatabase {\n\n\t@PropertyMapping(skip = SkipPropertyMapping.ON_DEFAULT_VALUE)\n\tReplace replace() default Replace.ANY;\n\n\tEmbeddedDatabaseConnection connection() default EmbeddedDatabaseConnection.NONE;\n\n\tenum Replace {\n\t\tANY,\n\t\tAUTO_CONFIGURED,\n\t\tNONE\n\t}\n}\n```\n\n- `replace()`:  대체할 수 있는 기존 DataSource 빈의 유형을 결정한다.\n    - `Replace.ANY`: 자동 구성 또는 수동 정의의 여부에 상관 없이 DataSource를 교체한다. default 설정 이기 때문에 `@DataJpaTest`를 사용하면 기본적으로 `in-memory embedded database`를 활용한다.\n    - `Replace.AUTO_CONFIGURED`: 자동 설정된 경우에만 DataSource를 교체한다.\n    - `Replace.NONE`: 기본 DataSource를 교체하지 않는다. 즉 우리가 직접 빈으로 등록하거나 명시한 DataSource를 사용한다. 만약 `in-memory embedded database`가 아닌 외부 DB나 테스트 용 DB를 사용하고 싶다면 `@AutoConfigureTestDatabase(replace = Replace.NONE)`으로 설정을 덮어야 한다.\n\n#### @Transactional\n\n```java\n...\n@Transactional\n...\npublic @interface DataJpaTest {\n\t...\n}\n```\n\n앞서 언급한 것 처럼 `@DataJpaTest`는 기본적으로 `@Transactional` 애노테이션을 들고 있기 때문에 테스트가 완료되면 자동으로 롤백된다.\n\n아래는 실제 프로젝트를 진행하며 작성한 테스트 코드의 일부를 가져온 것이다.\n\n```java\n@DataJpaTest\n@Import(JpaConfig.class)\nclass ScheduleRepositoryTest {\n\n    @Autowired\n    private ScheduleRepository scheduleRepository;\n\n    @DisplayName(\"시작일시와 종료일시를 전달하면 그 사이에 해당하는 일정을 조회한다.\")\n    @Test\n    void 시작일시와_종료일시를_전달하면_그_사이에_해당하는_일정을_조회한다() {\n        // given\n        Schedule schedule1 = new Schedule(TITLE, LocalDateTime.of(2022, 7, 14, 14, 20),\n                LocalDateTime.of(2022, 7, 15, 16, 20), MEMO);\n\n        Schedule schedule2 = new Schedule(TITLE, LocalDateTime.of(2022, 8, 15, 14, 20),\n                LocalDateTime.of(2022, 8, 15, 16, 20), MEMO);\n\n        scheduleRepository.save(schedule1);\n        scheduleRepository.save(schedule2);\n\n        // when\n        List<Schedule> schedules = scheduleRepository.findByBetween(START_DAY_OF_MONTH, END_DAY_OF_MONTH);\n\n        // then\n        assertThat(schedules).hasSize(1);\n    }\n    ...\n}\n```\n\n## 정리\n\n이 밖에도 `@JdbcTest`, `@DataMongoTest`, `@RestClientTest` 등 다양한 슬라이스 테스트를 위한 애노테이션이 제공된다. 어떠한 애노테이션을 사용하는 것에 집중하기 보다 `테스트의 목적`에 대해 고민해야 한다. 테스트하고자 하는 것에 집중하여 의존하거나 필요한 빈들에 대해 고민한 뒤 적절한 애노테이션을 적용하면 불필요한 리소스를 줄일 수 있으며 보다 더 빠른 테스트 피드백을 확인할 수 있을 것이라 판단한다.\n\n## References.\n[Spring Boot Test](https://meetup.toast.com/posts/124)<br>\n[8. Testing](https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.testing)<br>\n[Spring Boot 슬라이스 테스트](https://tecoble.techcourse.co.kr/post/2021-05-18-slice-test/)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. 글을 쓰게 된 계기 우아한테크코스 레벨3 프로젝트를 진행하면서 클라이언트와 서버의 통신에 SSL 인증서를 적용하여 HTTPS 통신을 하도록 하였다.\nHTTPS의 통신과정에 대한 이해를 공유하기 위해 글을 작성하고자 한다. HTTPS 는 SSL(Secure Sokect Layer) 위에 를 얹어서 보안이…","fields":{"slug":"/ssl_protocol/"},"frontmatter":{"date":"July 31, 2022","title":"SSL을 통한 HTTPS통신 과정","tags":["HTTPS"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n## 글을 쓰게 된 계기\n우아한테크코스 레벨3 프로젝트를 진행하면서 클라이언트와 서버의 통신에 SSL 인증서를 적용하여 HTTPS 통신을 하도록 하였다.\nHTTPS의 통신과정에 대한 이해를 공유하기 위해 글을 작성하고자 한다.\n\n\n## HTTPS\n`HTTPS`는 SSL(Secure Sokect Layer) 위에 `HTTP`를 얹어서 보안이 보장된 통신을 하고자하는 프로토콜이다.\n\n## SSL 인증서\n클라이언트와 웹 서버간의 통신을 제3자가 보증해주는 전자문서이다.\n클라이언트가 서버에 접속한 직후에 서버는 클라이언트에게 `SLL 인증서`를 전달한다.\n클라이언트는 신뢰 할 수 있는 인증서인지 검증한 후 서버와 통신을 진행한다.\n(밑에서 자세히 설명)\n\n## SSL 인증서 사용이유\n왜 HTTP 통신에 굳이 SSL 인증 과정을 붙혓을까? 이유는 다음과 같다.\n- 통신 내용이 악의적인 목적을 가진 사람에게 노출되는 것 을 막을 수 있다.\n- 클라이언트가 접속하는 서버가 신뢰할 수 있는 서버인지 판단 할 수 있다.\n- 통신 내용의 악의적인 변경을 방지 할 수 있다.\n\n\nSSL 통신과정은 꽤나 복잡하다. \n그래서 SSL 프로토콜을 이해하려면 우선 `암호화/복호화`와 `키(대칭키, 공개키)`에 대해 이해할 필요가 있다.\n\n## 암호화, 복호화, 키\n어떤 정보를 외부에 노출시키지 않기 위해 변형하는 것을 `암호화` 라고한다.\n반대로 암호화된 데이터를 원본으로 복원하는것을 `복호화`라고 한다.\n암호화와 복호화에서 데이터 변형을 위해 사용하는것을 `키`라고 한다.\n\n이렇게 키를 사용하여 `암호화/복호화` 하는 방식에는 `대칭키 방식`과 `공개키 방식`이 있다. \n두 방식 모두 `SSL 통신`에 사용되기 때문에 이해할 필요가 있다.\n\n\n## 대칭키 방식\n대칭키는 동일한 키로 `암호화`와 `복호화`를 같이 할 수 잇는 방식의 암호화 기법을 의미한다.\n`암호화`와 `복호화`를 위해 양쪽이 같은 키를 가져야 한다는 점에서 이 키를 `대칭키`라고 한다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/01b1f9b4-66ed-46d6-87d4-b18948611b80/image.JPG)\n\n전쟁상황을 가정해서 살펴보자. `A진영`과 `B진영`이 전쟁 중일 때, 적군에게 정보를 노출하지 않으려면 `서로 같은 키(대칭키)`를 가지고 정보를 암호화 해서 전달 할 수 있을 것이다.\n\n그런데, `대칭키`는 하나의 큰 단점을 가진다. 바로 `대칭키`를 `A진영`에서 `B진영` 또는 `B진영`에서 `A진영`으로 전달 할 때, \n`대칭키`가 탈취 된다면 적군도 정보를 복호화 할 수 있다는 것이다.\n\n이런 배경에서 나온 암호화 방식이 `공개키 방식`이다.\n\n## 공개키 방식\n`공개키`는 키가 두개가 있다.\nA키로 암호화 하면 B키로 복호화 할 수 있고 B키로 암호화 하면 A키로 복호화 하는 방식이다.\n\n이 방식에 착안해서 두개의 키 중 하나를 공개키, 하나를 비공개 키로 지정한다. \n\n그림을 통해 자세히 살펴보자.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/58d5be37-7ee5-4d8f-aacb-8ea66c23ecda/image.JPG)\n\n`A진영`은 `공개키`를 `B진영`에게 전달한다.\n그리고 `B진영`은 `A진영`의 `공개키`를 통해 정보를 암호화 한 후 `A진영`에 전달한다.\n`A진영`은 `비공개키`를 통해 전달 받은 정보를 복호화한다.\n\n`A진영`의 공개키가 탈취되거나 `B진영`이 전달한 정보가 탈취되어도 `비공개키`가 없으면 복호화가 불가능 하기 때문에 `대칭키 방식`의 단점을 극복 할 수 있다.\n\n`공개키 방식`은 이렇게 응용 할 수 있다.\n`A진영`이 `비공개키`를 이용해서 정보를 암호화 한 후에 공개키와 함께 암호화된 정보를 `B진영`에 전달한다.\n\n그런데, 이런 응용방식은 중간에 `공개키`와 `암호화된 정보`를 탈취 당하면 적군에게 정보를 노출 당할 수 있는 문제를 가져온다.\n\n그런데 `SSL 통신`에서는 이런 응용방식을 사용한다. 그 이유가 무엇일까?\n\n## SSL 통신이 공개키 응용방식을 사용하는 이유\n그 이유는 `공개키 응용방식`이 데이터 보호 목적이 아니기 때문이다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/bdfe5ab8-c202-4d3a-bd6a-34b1e46b37c3/image.JPG)\n\n`A진영`이 전달한 정보를 `A진영의 공개키`로 복호화 할 수 있다면, 정보가 `A진영`이 전달한 정보임을 신뢰 할 수 있다.\n\n즉, 공개키가 정보를 전달한 사람의 신원을 보장해주는 것이다. 이것을 전자 서명이라고 하고 `SSL 통신`에서 서버의 신원 확인을 위해 사용한다.\n\n\n## SSL 용어 정리\n\n이제 SSL 통신에 사용되는 `대칭키 방식`, `공개키 방식`, `공개키 응용방식`을 살펴보았으니 SSL을 통한 통신에 대해 자세히 살펴보자. \n우선, SSL과 관련된 용어를 정리해보자.\n\n### CA\nSSL 인증서의 역할은 클라이언트가 접속한 서버가 클라이언트가 의도한 서버가 맞는지를 보장하는 역할을 한다. \n이 역할을 하는 민간기업들이 있는데 이런 기업들을 `CA(Certificate authority)` 라고한다.\n\n\n### SSL 인증서 내용\n`SSL 인증서`에는 다음과 같은 정보가 포함되어 있다.\n\n- 서비스의 정보 (인증서를 발급한 CA, 서비스의 도메인 등등)\n- 서버 측 공개키 (공개키의 내용, 공개키의 암호화 방법)\n\n## SSL 통신 과정\n이제 `키를 사용한 3가지 통신 방법`을 살펴보았으니 SSL 통신과정을 이해해보자.\n\n`SSL 통신과정`에서 가장 이상적인 방법은 `공개키 방식`이다.(공개키 응용방식 아님)\n![](https://velog.velcdn.com/images/gudonghee2000/post/b528a5c7-9515-4a8b-a4d8-f1341abf57d2/image.JPG)\n\n\n`클라이언트`와 `서버`는 각각의 `비공개키, 공개키`를 가지고\n자신의 `공개키`를 서로에게 전달한다. 이때, `공개키`는 탈취되어도 `비공개키`가 없으면 복호화가 불가능하기 때문에 보안에 취약점이 없다.\n\n그리고, `클라이언트`는 `서버`의 `공개키`로 정보를 암호화 해서 전달하고 `서버`는 전달 받은 정보를 자신의 `비공개키(서버 비공개키)`로 복호화한 뒤 정보를 처리한다.\n\n마지막으로 `서버`는 `클라이언트의 공개키`로 응답해줄 정보를 암호화해서 다시 클라이언트에게 전달한다.\n\n그런데, `공개키 방식`은 컴퓨팅 파워를 많이 쓴다고한다.(컴퓨팅 파워를 많이쓰는 정확한 이유는 모름) 그래서 성능측면에서 비효율적이다.\n\n결론적으로, `SSL 통신`은 `공개키 방식 + 대칭키 방식`을 사용하여 보안과성능 두가지 측면을 보장한다.\n\n이제 SSL 통신과정을 천천히 자세하게 살펴보자.\n\n> HandShake -> 통신 -> 통신종료\n\n간단하게, SSL 통신은 위 세가지 과정으로 이루어진다.\n순서대로 하나씩 살펴보자.\n\n### HandShake(악수)\nSSL 통신은 데이터를 주고 받기전에 `어떻게 데이터를 암호화 할지`, `믿을 만한 서버인지` 등에 대해 이 과정에서 확인한다.\n\n1. 클라이언트가 서버에 접속한다. 이 단계를 `Client Hello`라고 한다. 이 단계에서 주고 받는 정보는 아래와 같다.\n  \n    - 클라이언트 측에서 생성한 `랜덤 데이터` (밑에서 설명)\n  \n    - 클라이언트가 지원하는 암호화 방식들 => 클라이언트가 가능한 암호화 방식을 서버에 알려주기 위함\n  \n2. 서버는 `Client Hello`에 대한 응답으로 `Server Hello`를 하게 된다. 이 단계에서 주고 받는 정보는 아래와 같다.\n  \n    - 서버 측에서 생성한 랜덤 데이터 (밑에서 설명)\n  \n    - 서버가 선택한 클라이언트의 암호화 방식 => 선택한 암호화 방식을 클라이언트에게 알려주기 위함\n  \n    - 인증서\n  \n  \n3. `클라이언트`는 `서버의 인증서`가 `CA`에 의해 발급된 것인지 확인한다.\n  이때, `클라이언트`에 내장된 `CA리스트와 CA의 공개키`를 사용해서 인증서를 복호화 한다. \n>   💡 참고\n  위에서 말한 공개키 응용방식의 활용\n  그리고, 클라이언트(브라우저)는 CA 종류와 CA의 공개키가 내장되있음\n  \n  성공적으로 인증서가 복호화 됬다면, `서버`가 전달한 인증서가 `CA의 개인키`로 암호화된 문서임이 보증된 것이다.\n**  즉, 올바른 서버임을 신뢰 할수 있게된다.**\n  \n  서버를 신뢰할수 있으므로 클라이언트는 `서버가 생성한 랜덤 데이터`와 `클라이언트가 생성한 랜덤 데이터`를 조합하여 `pre master secret`이라는 키를 생성한다. \n  \n 이때, `pre master secret`키는 대칭키 방식으로 사용 할 것이다.\n 클라이언트와 서버가 동일하게 가지고 데이터를 암호화/복호화 하는데 사용한다는 것이다.\n \n 그런데 대칭키 방식의 단점에서 살펴봤듯이 `pre master secret`를 그대로 서버에 전달하면 중간에 탈취당해 악용될수 있다.\n \n 이때, 사용하는 방법이 `공개키 방식`이다.\n `서버의 공개키(서버가 전달해준 인증서 내부에 들어있었음)`로 `pre master secret`를 암호화해서 `서버`로 전송한다.\n\n\n \n 4. `서버`는 자신의 비공개키를 통해 `pre master secret`를 복호화한다.\n \n 이를 통해, 클라이언트와 서버는 안전하게 같은 `pre master secret`를 가진다.\n 서버와 클라이언트는 일련의 과정을 통해 `pre master secret`을 `master secret`라는 `session key`를 생성한다.\n`master secret`은 실제로 클라이언트와 서버가 주고받는 데이터를 `암호화/복호화` 하는데 사용한다.\n\n5. 클라이언트와 서버는 `HandShake`가 종료됬음을 서로에게 알린다.\n\n\n### 통신\n이제 `master secret`을 통해 `클라이언트`와 `서버`는 데이터를 `암호화/복호화`하면서 주고 받는다. \n\n### 통신종료\n데이터의 전송이 끝나면 SSL 통신이 끝났음을 서로에게 알려준다.\n그리고 사용한 대칭키인 `master secret`은 폐기한다.\n \n \n## 마치면서\n이상 SSL통신에 대해서 자세하게 살펴보았다.\nSSL 통신은 `공개키 방식`, `대칭키 방식`을 합쳐서 사용하는 만큼 꽤나 복잡하다. \n작성한 글이 많은 크루들에게 HTTPS와 SSL 통신에 대한 이해에 도움이 되었으면 좋겠다.\n\n## 참조\nhttps://opentutorials.org/course/228/4894"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. 글을 쓰게 된 계기 우테코 레벨3 프로젝트를 진행하면서, HTTPS 통신을 적용하기 위해 NGINX에 대한 이해가 필요했다.\n그래서 NGINX에 대한 이해를 돕기위해 글을 작성하고자 한다. NGINX란? Nginx는 WS(Web Server)의 일종이다. 주로 정적 컨텐츠를 제공하거나 ReversePr…","fields":{"slug":"/what_is_nginx/"},"frontmatter":{"date":"July 30, 2022","title":"NGINX 란?","tags":["WS","NGINX"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n## 글을 쓰게 된 계기\n우테코 레벨3 프로젝트를 진행하면서, HTTPS 통신을 적용하기 위해 NGINX에 대한 이해가 필요했다.\n그래서 NGINX에 대한 이해를 돕기위해 글을 작성하고자 한다.\n\n<br>\n\n## NGINX란?\n> _Nginx는 WS(Web Server)의 일종이다. 주로 정적 컨텐츠를 제공하거나 ReverseProxy, LoadBalancer의 역할을 한다._\n\n<br>\n\n## NGINX의 등장 배경\nNGINX를 살펴보기전에, 무슨 이유로 NGINX가 등장했는지를 먼저 살펴볼 필요가 있다. 우선 NGINX의 등장배경을 살펴보자.\n\n<br>\n\nNGINX 등장이전 Apache가 웹서버로써의 높은 인기를 가졌다.\n잘 사용되던 Apache 웹서버는 어느순간 어떠한 문제를 가져왔다. Apache의 요청처리 메커니즘과 함께 어떠한 문제를 가져왔는지 살펴보자. \n![](https://velog.velcdn.com/images/gudonghee2000/post/4c31d7af-155b-4998-9ef3-93559914949e/image.JPG)\n\n\n초기의 Apache는 그림과 같이 요청(Request)이 들어올 떄 마다 새로운 Process를 생성하여 네트워크 연결을 하고 요청을 처리했다.(이러한 처리방식을 `prefork`라고 함)\n\n그런데, 2000년에 접어 들고 인터넷 트래픽이 증가하면서 아파치의 요청 처리방식은 `C10K` 문제를 가져왔다.\n> \nC10K 문제란 Connection 10000 Problem의 줄임말로 웹서버가 1만개의 동시 Connection을 처리하기 어렵다는 의미이다.\n\n\n\n일단 동시 Connection을 자세히 살펴보자.\n![](https://velog.velcdn.com/images/gudonghee2000/post/954a297f-e564-43b1-acfa-0f2452854647/image.JPG)\n\n\n`웹서버`는 `클라이언트`로 부터 요청이 들어오면 `Connection`을 생성하고 유지한다.\n그리고 그림과 같이 `클라이언트`는 생성된 `Connection`을 통해 또다른 요청을 `서버`에게 전달한다. \n\n이렇게 `Connection` 하나로 여러 요청을 처리하는 이유는 다음과 같다. 클라이언트와 서버는 `Connection`을 생성하는데 여러가지 절차가 필요하다. \n그래서 매 요청마다 `Connection`을 생성하는것은 비효율적이고 느렸다. \n\n비효율성을 해결하기 위해 사람들은 이미 만들어진 `Connection`이 있다면 이를 활용하여 요청을 보내고자 하였다.\n\n이렇게 유지되는 `Connection`들을 `동시 Connection`이라고 한다.\n\n> 💡 여담으로 HTTP 프로토콜 `Header`부분의 `Keep-Alive`가 바로 `Connection`을 얼마나 유지할 것인지에 대한 통신 규약이다.\n\n이때, Apache 서버는 C10K문제를 가져왔다.\n![](https://velog.velcdn.com/images/gudonghee2000/post/fa69475a-0326-4652-b1da-616e60164dea/image.JPG)\n\n그림과 같이 `Apache`서버는 요청이 들어올 때 마다, `Process`를 생성했는데 요청이 만단위를 넘어가면서 어느순간 부터 요청에 대한 `Connection`을 생성하지 못한것이다. \n\n이러한 문제를 가져온 원인은 다음과 같았다.\n-  메모리 부족\nApache서버는 `Connection`이 생성될 때마다 `Process`를 생성해야했는데 동시에 유지해야할 `Connection`이 많아지면서 유지해야할 `Process`가 증가했고 메모리 부족을 발생시켰다.\n\n- CPU 과부하\n또한, 실행중인 `Process`가 많아지면서 CPU는 `Process`를 처리 할 때, `컨텍스트 스위칭`을 굉장히 많이해야했다. 그래서 CPU의 부하가 증가했다.\n\n결국, 수많은 동시 커넥션을 처리하기엔 Apache의 요청 처리구조는 부적합했다.\n\n그래서 Apache의 단점을 보완하기 위해 2004년 NGINX가 등장했다.\n\n<br>\n\n## NGINX 자세히 알아보기\n\n우선, NGINX의 요청 처리 방식을 살펴보자.![](https://velog.velcdn.com/images/gudonghee2000/post/93b0890b-c015-46d0-9165-5cf6a7b4d9f6/image.JPG)\n\nNGINX는 `MasterProcess`를 통해 설정 파일을 읽고 `WorkerProcess`와 같은 자식 `Process` 3종류를 생성한다. (`cache loader`, `cache manager`가 있음)\n\n그리고 생성된 `WorkerProcess`는 요청이 들어오면 `Connection`을 형성하고 요청을 처리한다.\n요청에 따라서 매번 `Process`를 생성하던 Apache와 달리, NGINX는 `MasterProcess`에 따라 `WorkerProcess`를 생성하고 고정된 개수의 `WorkerProcess` 들이 요청을 처리한다.\n\n이렇게 고정된 `Process`의 개수로 요청을 처리하기 위해 NGINX는 `Event-Driven` 방식으로 요청을 처리한다.\n\n> 💡 Event-Driven이란?\nNGINX는 형성된 Connection에 아무런 요청이 없으면 새로운 요청에 대한 Connection을 형성하여 요청을 처리한다. \n또는 이미 만들어진 다른 Connection으로부터 요청을 처리한다. \nNginx에서의 Conneciton 형성, Connection 제거, 새로운 요청 처리를 Event라고 부른다.\n또한, Event를 비동기 방식으로 처리하는 것을 Event-Driven이라고 한다.\n\n<br>\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/f1b98399-de8f-4f10-bbba-050ca377c4be/image.JPG)\n\nNGINX는 그림과 같이 큐형태의 저장소에 Event들을 담아 `WorkerProcess`가 순차적으로 작업을 처리한다.\n\n<br>\n\n## NGINX의 Apache의 차이점\nApache는 동기방식으로 하나의 `Connection`이 끝날 때 까지 `Process`를 유지했다. \n\n반면, NGINX는 고정된 `WokerProcess`를 생성하고 `Event`가 발생 할 때마다 요청을 처리하는 비동기 `Event-Driven` 방식을 사용한다는 것이 차이점이다.\n\n보통 `WorkerProcess`는 CPU의 코어 개수만큼 생성하는것이 일반적이라고 한다. **(이유: 컨텍스트 스위칭 회수를 최소하 하기 위함.)**\n\n<br>\n\n## NGINX의 장점\nNGINX는 Apache에 비해 성능 측면에서 두가지 장점이있다.\n\n1. 처리할수 있는 동시 커넥션 개수가 훨씬 많다.\n\n2. 동일한 개수의 커넥션 처리 속도가 더 빠르다.\n\n이러한 장점을 가지고 오는 이유를 마지막으로 정리해보자.\n- 고정된 `Process` 개수 만을 사용하기에 `Process` 생성 비용이 없다.\n- 또한, `Process` 개수가 제한되어 CPU의 부담이 줄어든다. (컨텍스트 스위칭이 적어지기 때문)\n- 비동기 방식이기에 `Process`가 쉬지않고 일을 할 수 있다.\n\n<br>\n\n## NGINX 활용 방법\n도입부에서 이야기햇듯이 NGINX는 정적파일처리, 리버스 프록시, 로드밸런서의 역할로써 WAS의 부담을 줄여주는 역할로 다양하게 활용 할 수 있다.\n\n<br>\n\n## 마치면서\n이상 NGINX의 등장 배경과 작동 방식을 살펴보았다.\n많은 크루들이 우아한테크코스 프로젝트를 진행하면서 사용하는 NGINX에 대한 이해에 도움이 되었으면 좋겠다.\n\n<br>\n\n### 참조\nhttps://livlikwav.github.io/study/NGINX-inside/\nhttps://jizard.tistory.com/306\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. 문제 에서 카테고리 수정 기능에 대한 테스트가 터졌다.  시 다른 회원의 id를 넣으면 이 발생해야 하는데 아무런 예외도 발생하지 않았다. 더 이상한 점은 단독으로 돌렸을 때는 잘 돌아가지만 전체 테스트를 돌리면 터진다는 것이다. 테스트 격리에 문제가 있어 보였다. 디버깅을 해보았다.  MEMBER까지 저…","fields":{"slug":"/test-fixture-constant/"},"frontmatter":{"date":"July 27, 2022","title":"테스트에서 Entity 객체를 상수로 두면 안 되는 이유","tags":["Spring","test fixture","상수"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n## 문제\n\n```java\n// MemberFixtures\n\npublic static final Member MEMBER \n\t\t= new Member(\"example@email.com\", \"/image.png\", \"example\", SocialType.GOOGLE);\npublic static final Member CREATOR \n\t\t= new Member(\"creator@email.com\", \"/image.png\", \"creator\", SocialType.GOOGLE);\n```\n\n```java\n// CategoryServiceTest\n\n@DisplayName(\"자신이 만들지 않은 카테고리를 수정할 경우 예외를 던진다.\")\n@Test\nvoid 자신이_만들지_않은_카테고리를_수정할_경우_예외를_던진다() {\n    // given\n    Member member = memberRepository.save(MEMBER);\n    Member creator = memberRepository.save(CREATOR);\n    CategoryResponse savedCategory = categoryService.save(creator.getId(),\n            new CategoryCreateRequest(CATEGORY_NAME));\n\n    CategoryUpdateRequest categoryUpdateRequest = new CategoryUpdateRequest(MODIFIED_CATEGORY_NAME);\n\n    // when & then\n    assertThatThrownBy(\n            () -> categoryService.update(member.getId(), savedCategory.getId(), categoryUpdateRequest))\n            .isInstanceOf(NoPermissionException.class);\n}\n```\n\n`CategoryServiceTest`에서 카테고리 수정 기능에 대한 테스트가 터졌다. `update` 시 다른 회원의 id를 넣으면 `NoPermissionException`이 발생해야 하는데 아무런 예외도 발생하지 않았다. 더 이상한 점은 단독으로 돌렸을 때는 잘 돌아가지만 전체 테스트를 돌리면 터진다는 것이다. 테스트 격리에 문제가 있어 보였다. 디버깅을 해보았다.\n\n![member 객체](debug1.png)\n\nMEMBER까지 저장했을 때는 분명 값들이 잘 들어가있는 걸 볼 수 있다.\n\n![필드가 변경된 member 객체](debug2.png)\n\n근데 CREATOR를 저장하는 순간!? member의 값이 갑자기 CREATOR의 값으로 전부 변경되었다. 왜 이런 걸까.\n\n## 원인\n\n> 가변객체인 회원의 Entity를 상수로 등록하여 사용하고 있는 것이 문제였다.\n\n아래 그림을 통해 알아보자.\n\n![](img1.png)\n\n테스트 실행 전 MEMBER와 CREATOR는 id를 가지지 않은 상태이다. 영속성 컨텍스트도 비어있다.\n\n전체 테스트를 실행하면 MEMBER와 CREATOR는 상수이므로 온갖 테스트에서 두 개의 상수를 가져다 쓰게 된다. 그 과정에서 MEMBER가 저장되는 상황이 발생한다.\n\n![](img2.png)\n\n그럼 상수 MEMBER의 id 값이 1로 바뀌게 된다. 이후 다른 테스트에서는 CREATOR가 저장된다. 이 때 각각의 테스트는 격리되어 있으므로 영속성 컨텍스트는 빈 상태이다. 따라서 CREATOR의 id도 1로 저장된다.\n\n![](img3.png)\n\n이렇게 상수 MEMBER와 CREATOR의 id가 모두 1로 바뀌었다. 디버깅을 해보니 실제로 상수의 id 값이 모두 1로 바뀐 것을 볼 수 있다.\n\n![](debug3.png)\n\n이 상태에서 처음 봤던 테스트 코드로 돌아가보자.\n\n```java\n// CategoryServiceTest\n\n@DisplayName(\"자신이 만들지 않은 카테고리를 수정할 경우 예외를 던진다.\")\n@Test\nvoid 자신이_만들지_않은_카테고리를_수정할_경우_예외를_던진다() {\n    // given\n    Member member = memberRepository.save(MEMBER);\n    Member creator = memberRepository.save(CREATOR); // 문제 발생 !!!\n    CategoryResponse savedCategory = categoryService.save(creator.getId(),\n            new CategoryCreateRequest(CATEGORY_NAME));\n\n    CategoryUpdateRequest categoryUpdateRequest = new CategoryUpdateRequest(MODIFIED_CATEGORY_NAME);\n\n    // when & then\n    assertThatThrownBy(\n            () -> categoryService.update(member.getId(), savedCategory.getId(), categoryUpdateRequest))\n            .isInstanceOf(NoPermissionException.class);\n}\n```\n\n테스트를 보면 처음으로 MEMBER를 저장한다. 이까지는 아무 문제가 없다. 문제는 두 번째로 CREATOR를 저장할 때 발생한다.\n\n![](img4.png)\n\nCREATOR를 저장하려고 하는데 저장된 MEMBER의 id도 1이고, CREATOR의 id도 1이다. CREATOR를 저장할 때 영속성 컨텍스트에 같은 id를 가진 entity가 있으므로 CREATOR를 새롭게 저장하지 않고, 기존의 id가 1번인 entity를 수정하게 된다.\n\n![](img5.png)\n\n기존의 값이 수정되니 id가 1번인, MEMBER를 저장한 member instance의 값까지 변경되는 것이다.\n\n## 해결\n\nEntity 객체에 대한 fixture가 필요한 경우 객체를 생성하는 메서드를 통해 호출할 때마다 새로운 객체를 생성해서 반환하도록 했다.\n\n```java\npublic static Member 파랑() {\n    return new Member(파랑_이메일, 파랑_프로필, 파랑_이름, SocialType.GOOGLE);\n}\n```\n\n## 결론\n\n상수란 변하지 않고 고정된 값을 담는 변수를 의미한다. 하지만 우리는 필드가 변하는 객체인 Member Entity를 상수로 선언했다. 테스트 격리를 아무리 잘 해도 상수까지 계속 리프레쉬 해주진 않기 때문에 이런 문제가 발생한 것이다.\n\n**변하지 않는 불변 객체만 상수로 지정하자!!**\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. properties 객체로 다루기 Spring에서 이나 에 존재하는 값을 불러오는 방법에는 대표적으로  애노테이션을 사용한 방법과 를 사용한 방법이 존재한다. 두 방식을 직접 적용해 본 뒤 차이와 이점에 대해 알아보려 한다. @Value 사용하기 는 기본적으로 설정 정보를 단일값으로 주입 받기 위해 사…","fields":{"slug":"/properties-to-object/"},"frontmatter":{"date":"July 27, 2022","title":"properties 객체로 다루기","tags":["properties","Value","ConfigurationProperties"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## properties 객체로 다루기\n\nSpring에서 `application.yml`이나 `application.properties`에 존재하는 값을 불러오는 방법에는 대표적으로 `@Value` 애노테이션을 사용한 방법과 `@ConfigurationProperties`를 사용한 방법이 존재한다. 두 방식을 직접 적용해 본 뒤 차이와 이점에 대해 알아보려 한다.\n\n## @Value 사용하기\n\n`@Value`는 기본적으로 설정 정보를 단일값으로 주입 받기 위해 사용된다. 아래는 실제 달록 프로젝트에서 적용한 예시이다.\n\n```java\n@Component\npublic class GoogleOAuthClient implements OAuthClient {\n\n    private static final String JWT_DELIMITER = \"\\\\.\";\n\n    private final String clientId;\n    private final String clientSecret;\n    private final String grantType;\n    private final String redirectUri;\n    private final String tokenUri;\n    private final RestTemplate restTemplate;\n    private final ObjectMapper objectMapper;\n\n    public GoogleOAuthClient(@Value(\"${oauth.google.client-id}\") final String clientId,\n                             @Value(\"${oauth.google.client-secret}\") final String clientSecret,\n                             @Value(\"${oauth.google.grant-type}\") final String grantType,\n                             @Value(\"${oauth.google.redirect-uri}\") final String redirectUri,\n                             @Value(\"${oauth.google.token-uri}\") final String tokenUri,\n                             final RestTemplateBuilder restTemplateBuilder, final ObjectMapper objectMapper) {\n        this.clientId = clientId;\n        this.clientSecret = clientSecret;\n        this.grantType = grantType;\n        this.redirectUri = redirectUri;\n        this.tokenUri = tokenUri;\n        this.restTemplate = restTemplateBuilder.build();\n        this.objectMapper = objectMapper;\n    }\n\t\t...\n}\n```\n\n간단하게 적용이 가능하지만 공통으로 묶인 프로퍼티가 많아질 경우 코드가 지저분해진다. 이러한 프로퍼티 값들을 객체로 매핑하여 사용하기 위한 애노테이션으로 `@ConfigurationProperties`가 존재한다.\n\n## @ConfigurationProperties\n\n우리는 때때로 DB 설정을 작성하기 위해 `application.yml`을 통해 관련 정보를 작성하곤 한다. 아래는 간단한 h2 DB를 연결하기 위한 설정을 적은 예시이다.\n\n```yaml\nspring:\n  datasource:\n    url: jdbc:h2:~/dallog;MODE=MYSQL;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE\n    username: sa\n```\n\n이러한 설정들은 어디서 어떻게 활용되고 있을까? 실제 바인딩 되고 있는 객체를 따라가보자.\n\n```java\n@ConfigurationProperties(prefix = \"spring.datasource\")\npublic class DataSourceProperties implements BeanClassLoaderAware, InitializingBean {\n\t\n    private ClassLoader classLoader;\n    private boolean generateUniqueName = true;\n\tprivate String name;\n\tprivate Class<? extends DataSource> type;\n\tprivate String driverClassName;\n\tprivate String url;\n    ...\n}\n```\n\n위 `DataSourceProperties`는 우리가 `application.yml`에 작성한 설정 정보를 기반으로 객체로 추출하고 있다. 이것은 Spring Boot의 자동설정으로 `DataSource`가 빈으로 주입되는 시점에 설정 정보를 활용하여 생성된다. \n\n간단히 디버깅을 진행해보면 Bean이 주입되는 시점에 아래와 같이 `application.yml`에 명시한 값들을 추출한 `DataSourceProperties`를 기반으로 생성하고 있다.\n\n![](debug-1.png)\n\n![](debug-2.png)\n\n정리하면 우리는 Spring Boot를 사용하며 자연스럽게 `@ConfigurationProperties`를 활용하여 만든 객체를 사용하고 있는 것이다.\n\n이제 우리가 작성한 설정 값을 기반으로 객체를 생성해서 활용해보자. 아래는 실제 프로젝트에서 사용하고 있는 `application.yml`의 일부를 가져온 것이다.\n\n```yaml\n...\noauth:\n  google:\n    client-id: ${GOOGLE_CLIENT_ID}\n    client-secret: ${GOOGLE_CLIENT_SECRET}\n    redirect-uri: ${GOOGLE_REDIRECT_URI}\n    oauth-end-point: https://accounts.google.com/o/oauth2/v2/auth\n    response-type: code\n    scopes:\n        - https://www.googleapis.com/auth/userinfo.profile\n        - https://www.googleapis.com/auth/userinfo.email\n    token-uri: ${GOOGLE_TOKEN_URI}\n    grant-type: authorization_code\n...\n```\n\n이것을 객체로 추출하기 위해서는 아래와 같이 작성해야 한다.\n\n```java\n@ConfigurationProperties(\"oauth.google\")\n@ConstructorBinding\npublic class GoogleProperties {\n\n    private final String clientId;\n    private final String clientSecret;\n    private final String redirectUri;\n    private final String oAuthEndPoint;\n    private final String responseType;\n    private final List<String> scopes;\n    private final String tokenUri;\n    private final String grantType;\n\n    public GoogleProperties(final String clientId, final String clientSecret, final String redirectUri,\n                            final String oAuthEndPoint, final String responseType, final List<String> scopes,\n                            final String tokenUri, final String grantType) {\n        this.clientId = clientId;\n        this.clientSecret = clientSecret;\n        this.redirectUri = redirectUri;\n        this.oAuthEndPoint = oAuthEndPoint;\n        this.responseType = responseType;\n        this.scopes = scopes;\n        this.tokenUri = tokenUri;\n        this.grantType = grantType;\n    }\n\n    public String getClientId() {\n        return clientId;\n    }\n\n    public String getClientSecret() {\n        return clientSecret;\n    }\n\n    public String getRedirectUri() {\n        return redirectUri;\n    }\n\n    public String getoAuthEndPoint() {\n        return oAuthEndPoint;\n    }\n\n    public String getResponseType() {\n        return responseType;\n    }\n\n    public List<String> getScopes() {\n        return scopes;\n    }\n\n    public String getTokenUri() {\n        return tokenUri;\n    }\n\n    public String getGrantType() {\n        return grantType;\n    }\n}\n```\n\n- `@ConfigurationProperties`: 프로퍼티에 있는 값을 클래스로 바인딩하기 위해 사용하는 애노테이션이다. `@ConfigurationProperties`는 값을 바인딩하기 위해 기본적으로 `Setter`가 필요하다. 하지만 `Setter`를 열어둘 경우 불변성을 보장할 수 없다. 이때 생성자를 통해 바인딩 하기 위해서는 `@ConstructorBinding`을 활용할 수 있다.\n- `@ConstructorBinding`: 앞서 언급한 것 처럼 생성자를 통해 바인딩하기 위한 목적의 애노테이션이다.\n\n```java\n@Configuration\n@EnableConfigurationProperties(GoogleProperties.class)\npublic class PropertiesConfig {\n}\n```\n\n- `@EnableConfigurationProperties`: 클래스를 지정하여 스캐닝 대상에 포함시킨다.\n\n### 개선하기\n\n```java\n@Component\npublic class GoogleOAuthClient implements OAuthClient {\n\n    private static final String JWT_DELIMITER = \"\\\\.\";\n\n    private final GoogleProperties googleProperties;\n    private final RestTemplate restTemplate;\n    private final ObjectMapper objectMapper;\n\n    public GoogleOAuthClient(final GoogleProperties googleProperties, final RestTemplateBuilder restTemplateBuilder,\n                             final ObjectMapper objectMapper) {\n        this.googleProperties = googleProperties;\n        this.restTemplate = restTemplateBuilder.build();\n        this.objectMapper = objectMapper;\n    }\n    ...\n}\n```\n\n이전 보다 적은 수의 필드를 활용하여 설정 정보를 다룰 수 있도록 개선되었다.\n\n### 정리\n\n 우리는 `application.yml` 혹은 `application.properties`에 작성하여 메타 정보를 관리할 수 있다. 클래스 내부에서 관리할 경우 수정하기 위해서는 해당 클래스에 직접 접근해야 한다. 하지만 설정 파일로 분리할 경우 우리는 환경에 따라 유연하게 값을 설정할 수 있다. 또한 `@ConfigurationProperties` 애노테이션을 사용할 경우 클래스로 값을 바인딩하기 때문에 연관된 값을 한 번에 바인딩할 수 있다.\n\n## References.\n\n[달록 repository](https://github.com/woowacourse-teams/2022-dallog)<br>\n[[Spring] @Value와 @ConfigurationProperties의 사용법 및 차이 - (2/2)](https://mangkyu.tistory.com/207)<br>\n[appendix.configuration-metadata.annotation-processor](https://docs.spring.io/spring-boot/docs/2.7.1/reference/html/configuration-metadata.html#appendix.configuration-metadata.annotation-processor)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. Rest Docs Spring Rest Docs는 테스트 코드 기반으로 자동으로 Rest API 문서를 작성 할 수 있도록 도와주는 프레임 워크이다. Rest Docs와 Swagger 자바 문서 자동화에는 주로 Rest Docs와 Swagger가 사용된다.\n각 자동화 프레임 워크의 장단점을 살펴보자.\n…","fields":{"slug":"/apply-rest-docs/"},"frontmatter":{"date":"July 26, 2022","title":"MockMvc를 사용한 Spring RestDocs","tags":["Spring","Rest API"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n## Rest Docs\nSpring Rest Docs는 테스트 코드 기반으로 자동으로 Rest API 문서를 작성 할 수 있도록 도와주는 프레임 워크이다.\n\n## Rest Docs와 Swagger\n자바 문서 자동화에는 주로 Rest Docs와 Swagger가 사용된다.\n각 자동화 프레임 워크의 장단점을 살펴보자.\n![](./compare.png)\n\nSwagger는 API 문서의 작성을 위해 프로덕션 코드에 추가적인 코드를 작성해야한다. \n그래서 Swagger의 사용은 프로덕션 코드의 가독성을 떨어트린다고 생각한다.\n\n반대로, Spring Rest Docs는 테스트 코드에 의존적이기 때문에 Spring Rest Docs를 사용하는것이 좋다고 생각한다.\n\n## MockMvc vs Rest Assured\nSpring Rest Docs를 사용하여 문서를 작성 하려면 테스트 코드가 필요하다.\n테스트 코드를 작성 할 때, 대표적으로 MockMvc와 Rest Assured를 사용한다.\n\nMockMvc를 사용하면  `@WebMvcTest`로 테스트 할 수 있다.\n그래서 Controller Layer만으로 테스트 하기 때문에 테스트 속도가 빠르다.\n\n반면, RestAssured는 `@SpringBootTest`로 수행해야한다. 그러면 전체 어플리케이션 컨텍스트를 로드하여 빈을 주입하기에 테스트 속도가 느리다.\n하지만, 실제 객체를 통한 테스트가 가능하기 때문에 테스트의 신뢰성이 높다.\n\n통합 테스트, 인수 테스트의 경우 RestAssuerd가 좋을 수 있지만, 문서를 작성하기 위한 테스트에는 MockMvc가 더 적절하다고 생각한다.\n\n**_💡 @WebMvcTest와 @SpringBootTest_**\n@WebMvcTest는 Application Context를 완전하게 Start하지 않고 Present Layer 관련 컴포넌트만 스캔하여 빈 등록한다.\n반면, @SpringBootTest의 경우 모든 빈을 로드하여 등록한다.\n\n\n## AsciiDoc\n\nSpring Boot Rest Docs는 Asciidoc를 문서 번역을 위한 텍스트 프로세서로 사용한다.\n\n## Rest Docs API 문서 생성 매커니즘\n우선, Rest Docs의 문서 생성 매커니즘을 살펴보자.\n\n1. MockMvc로 작성한 테스트 코드를 실행한다.\n\n2. 테스트가 통과하면 아래와 같이 `build/generated-snippets` 하위에 스니펫(문서조각)들이 생성된다. \n![](./first.png)\n\n   _❗❗ gradle은 build/generated-snippets에 스니펫이 생성된다._\n\n3. `build/generated-snippets` 하위에 생성된 스니펫들을 묶어서 HTML 문서를 만들기 위해서는, gradle의 경우 아래와 같이`src/docs/asciidoc` 하위에 스니펫들을 묶은 adoc문서를 만든다.![](./second.png)\n\n4. 스니펫을 이용해서 `src/docs/asciidoc` 하위에 adoc 파일을 생성했다면, `./gradlew build` 명령어를 통해 빌드를 해준다.\n![](./third.png)\n빌드가 완료되면 위와 같이 `resources - static - docs` 하위에 HTML 문서가 생성된다.\n\n5. 어플리케이션을 실행 한 후, `http://localhost:8080/docs/{HTML 파일명}` 을 웹브라우저에 검색하면 생성한 REST API 문서를 확인 할 수 있다. \n\n\t**❗❗ API문서 url은 코드를 통해 변경 가능하다.**\n    \n### ❗유의할 점\nresources - static - docs 하위의 HTML 파일은 실제로는 build.gradle의 설정파일에 따라서 위와같이 build - docs - asciidoc 하위의 HTML 파일을 복사해온 파일이다.\n![](./four.png)\n\n### 아이디어\nREST API 문서를 확인할 때, `http://localhost:8080/docs/{HTML 파일명}` 을 통해서 웹브라우저에 접근하지 않아도 확인하는 방법이 있다.\n![](./five.png)\nAsciiDoc 플러그인을 설치하면 위와같이, 인텔리제이 상에서도 REST API 문서를 실시간으로 확인할수 있다.  (✔설치 추천)\n\n## Rest Docs 사용을 위한 빌드파일 설정\n``` java\nplugins {\n    id 'org.asciidoctor.jvm.convert' version '3.3.2' // 1\n}\n\next {\n    snippetsDir = file('build/generated-snippets') // 2\n}\n\ntest { \n    outputs.dir snippetsDir // 3\n    useJUnitPlatform()\n}\n\nconfigurations {\n    asciidoctorExtensions\n}\n\nasciidoctor { // 4\n    configurations 'asciidoctorExtensions' \n    inputs.dir snippetsDir \n    dependsOn test\n}\n\ndependencies {\n    testImplementation 'org.springframework.restdocs:spring-restdocs-mockmvc' // 5\n    asciidoctorExtensions 'org.springframework.restdocs:spring-restdocs-asciidoctor' // 6\n}\n\ntask copyDocument(type: Copy) { // 7\n    dependsOn asciidoctor\n    \n    from file(\"build/docs/asciidoc\")\n    into file(\"src/main/resources/static/docs\")\n}\n\t\nbootJar { \n    dependsOn copyDocument // 8\n}\n  ```\n\n\n1. gradle7부터 사용하는 플러그인으로 asciidoc 파일 변환, build 디렉토리에 복사하는 플러그인이다.\n\n2. 생성된 스니펫을 저장할 위치를 정의한다. gradle은 `build/generated-snippets`에 스니펫이 생성된다.\n\n3. 테스트 Task의 결과 아웃풋 디렉토리를 `build/generated-snippets`로 지정한다.\n\n4. asciidoctor Task가 사용할 인풋 디렉토리를 `build/generated-snippets`로 지정한다.\n\tdependsOn test로 문서가 작성되기 전에 테스트가 실행되도록 한다.\n    \n5. MockMvc를 테스트에 사용하기 위한 의존성을 추가 해준다.\n\n6. 일반 텍스트를 처리하고 HTML 파일을 생성하는 의존성을 추가 해준다.\n\n7. asciidoctor Task로 생성한 `build/docs/asciidoc`파일을 `src/main/resources/static/docs`로 복사한다.\n\n8. bootJar 실행시 copyDocument를 먼저 실행하도록 한다.\n\n\n--- \n\n✅MockMvc를 사용한 Rest Docs 테스트 작성을 알아보기 전에 우선 MockMvc에 대해 알아보자.\n\n##MockMvc 기본 메서드\n어떠한 것들이 있는지 알아보고 밑에서 자세히 알아보자.\n\n### perform()\n가상의 request를 처리한다.\n\n```java\nmockMvc.perform(get(\"/api/schedules/?year=2022&month=7\"))\n```\n\n### andExpert()\nandExpert()\n\n예상값을 검증한다. \n\n```java\n.andExpect(status().isOk())\n// status 값이 정상인 경우를 기대하고 만든 체이닝 메소드의 일부\n\n.andExpect(content().contentType(\"application/json;charset=utf-8\"))\n//contentType을 검증\n```\n\n### andDo()\n요청에 대한 처리를 맡는다. print() 메소드가 일반적이다.\n\n```java\n.andDo(print())\n```\n\n### andReturn()\n테스트한 결과 객체를 받을 때 사용한다.\n\n```java\nMvcResult result = mockMvc.perform(get(\"/\"))\n.andDo(print())\n.andExpect(status().isOk())\n.andReturn();\n```\n\n## MockMvc 요청 만들기\n요청을 만들 때는 static 메서드인 get, post, put, delete, fileUpload 등을 이용해서 MockHttpServletRequestBuilder 객체를 생성하는 것에서 시작한다.\n\nMockHttpServletRequestBuilder는 ServletRequest를 구성하기에 필요한 다양한 메서드를 제공한다.\n![](./six.png)\n위 메서드들은 메서드 체이닝을 지원하기 때문에, 아래와 같이 요청 데이터를 연결해서 작성하면된다.\n\n\n```java \n@Test\n    void test() throws Exception {\n        MockHttpServletRequestBuilder builder = get(\"/api/schedules\")\n                .param(\"year\", \"2022\")\n                .param(\"month\", \"7\")\n                .accept(MediaType.APPLICATION_JSON)\n                .header(\"sessionId\", \"세션아이디입니다.\");\n\n        mockMvc.perform(builder)\n                .andExpect(status().isOk());\n    }\n\n```\n_**❗❗ 유의 할 점**_\nMockMvc.perform() 의 파라미터 값이 MockHttpServletRequestBuilder의 상위 객체이다. \n\n그래서 perform() 파라미터로 아래와 같이 넣어주어도 작동된다.\n```java\n@Test\n    void test() throws Exception {\n        mockMvc.perform(get(\"/api/schedules\")\n                .param(\"year\", \"2022\")\n                .param(\"month\", \"7\")\n                .accept(MediaType.APPLICATION_JSON)\n                .header(\"sessionId\", \"세션아이디입니다.\"))\n                .andExpect(status().isOk());\n    }\n```\n\n## MockMvc 실행 결과 검증\nperform()은 반환 값으로 ResultActions가 반환된다.\nResultActions의 andExpect는 요청 실행 결과를 검증 하려면 ResultMatcher를 넘겨줘서 검증해야한다.\nResultMatcher는 다음의 MockMvcResultMatchers가 가지는 static 메서드를 통해서 얻는다.\n\nMockMvcResultMatchers는 다음의 static 메서드를 통해 다양한 ResultMatcher를 제공한다.\n\n![](./seven.png)\n\n아래의 예시를 살펴보자.\n```java\n\t@Test\n    void test() throws Exception {\n        mockMvc.perform(builder)\n                .andExpect(handler().handlerType(ScheduleController.class))\n                .andExpect(handler().methodName(\"save\"))\n                .andExpect(forwardedUrl(\"index\"))\n                .andExpect(header().stringValues(\"Content-Language\", \"en\"))\n                .andExpect(model().attribute(\"message\", \"저장이 잘되었습니다.\"))\n                .andExpect(status().isOk());\n    }\n```\n\n## MockMvc 실행 결과 처리\n실행 결과를 출력할 떄는 andDo 메서드를 사용한다.\nandDo 메서드 의 인수에는 실행 결과를 처리 할 수 있는 ResultHandler를 지정한다.\nMockMvcResultHandlers는 다양한 ResultHandler를 제공하지만 print()를 주로 사용한다.\n\n\n## MockMvc를 사용한 Rest Docs 생성\n테스트 코드와 함께 MockMvc를 사용한 Rest Docs 생성을 알아보자.\n\n```java\n@WebMvcTest(ScheduleController.class)\n@AutoConfigureRestDocs // 1\nclass ScheduleControllerTest {\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @Autowired\n    private ObjectMapper objectMapper;\n\n    @MockBean // 2\n    private ScheduleService scheduleService;\n\n    @Test\n    void save() throws Exception {\n        // given\n        ScheduleCreateRequest request = new ScheduleCreateRequest(\"제목\", LocalDateTime.now(), LocalDateTime.now(), \"메모\");\n\n        given(scheduleService.save(request))\n                .willReturn(1L); // 3\n\n        // when & then\n        mockMvc.perform(post(\"/api/schedules\")\n                        .content(objectMapper.writeValueAsString(request))\n                        .contentType(MediaType.APPLICATION_JSON)\n                        .accept(MediaType.APPLICATION_JSON))\n                .andExpect(status().isOk())\n                .andDo(document(\"schedule-save\", // 4\n                        requestFields(\n                                fieldWithPath(\"title\").type(JsonFieldType.STRING).description(\"제목\"),\n                                fieldWithPath(\"startDateTime\").type(JsonFieldType.STRING)\n                                        .description(\"2022-07-04T13:00\"),\n                                fieldWithPath(\"endDateTime\").type(JsonFieldType.STRING).description(\"2022-07-05T07:00\"),\n                                fieldWithPath(\"memo\").type(JsonFieldType.STRING).description(\"메모입니다.\")\n                        )\n                ));\n    }\n}\n```\n\n1. target/generated-snippets dir 생성하고 테스트 코드를 통해 snippets를 추가해주는 애노테이션이다.\n\n2. `ScheduleService`를 mocking을 하기위해서 `@MockBean` 을 선언한다.\n\n3. mocking을 통해 `ScheduleService` 를 통해 받을 응답값을 설정한다.\n\n4. test 수행시 `andDo(document(\"xxx\"))`를 통해서 `./build/generated-snippets` 하위에 문서가 작성된다.\n\n\n---\n## 끝내면서 \n이상 Rest Docs의 매커니즘, 설정 그리고 MockMvc를 활용한 Rest Docs 생성 방법을 살펴보았다.\n프로젝트에 RestAssuered를 사용한 Rest Docs를 적용하면서 테스트 격리에 문제를 경험하였는데,\n테스트 격리에 대해서 추후에 포스팅 해봐야겠다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. 들어가기에 앞서 이전 포스팅에서 SQL 중심적인 개발의 문제점을 살펴보고 객체 중심적인 개발을 하기위한 JAVA의 JPA를 가볍게 언급해보았다. 그렇다면, JPA는 어떠한 매커니즘으로 JAVA에서 작동할까?\n이번 포스팅에서 JPA의 구동방식을 자세히 알아보자.   JPA 구동방식 우선, 아래의 그림을 …","fields":{"slug":"/mechanism-of-jpa/"},"frontmatter":{"date":"July 26, 2022","title":"JPA 작동 메커니즘","tags":["JPA"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n### 들어가기에 앞서\n이전 포스팅에서 SQL 중심적인 개발의 문제점을 살펴보고 객체 중심적인 개발을 하기위한 JAVA의 JPA를 가볍게 언급해보았다.\n\n그렇다면, JPA는 어떠한 매커니즘으로 JAVA에서 작동할까?\n이번 포스팅에서 JPA의 구동방식을 자세히 알아보자.  \n\n\n### JPA 구동방식\n우선, 아래의 그림을 통해 JPA의 구동방식을 가볍게 살펴보자.\n\n![](./jpa-mechanism.png)\n\n\n먼저, JPA는 Build 파일을 통해서 JPA 인터페이스를 구현할 **구현체 클래스인** `Persistence` 생성한다. \n**(JPA는 인터페이스이며 구현체로는 Hibernate, EclipseLink 등이 있음)**\n\n그리고 생성한 `Persistence`로 `META-INF/Persistence.xml`라는 설정파일의 정보를 읽어서 `EntityManagerFactory`라는 클래스를 생성한다.\n\n`EntityManagerFactory`는 필요할때 마다 `EntityManager` 라는 클래스를 생성한다. \n\n개발자는 `EntityManager`를 통해서 DB에 접근하고 CRUD 작업을 수행한다.\n\n위와 같은 과정을 통해 JPA를 사용 할 수 있다.\n\n그렇다면, `Persistence.xml`, `EntityManagerFactory`, `EntityManager`란 무엇일까?\n아래에서 자세히 살펴보자.\n\n### Persistence.xml란?\n`Persistence.xml`은 JPA가 엑세스하려는 데이터베이스들에 대해 필요한 설정정보 들을 기술해둔 파일이다.\nJPA는 이 파일의 설정정보를 바탕으로 접근할 DB의 정보를 가져 올 수 있다.\n\n`Persistence.xml` 파일에 대해 아래 그림과 함께 자세히 살펴보자.\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<persistence version=\"2.2\"\n \txmlns=\"http://xmlns.jcp.org/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_2.xsd\">\n\t<persistence-unit name=\"hello\">\n \t\t<properties>\n \t\t\t<!-- 필수 속성 -->\n \t\t\t<property name=\"javax.persistence.jdbc.driver\" value=\"org.h2.Driver\"/>\n \t\t\t<property name=\"javax.persistence.jdbc.user\" value=\"sa\"/>\n \t\t\t<property name=\"javax.persistence.jdbc.password\" value=\"\"/>\n\t\t\t<property name=\"javax.persistence.jdbc.url\" value=\"jdbc:h2:tcp://localhost/~/test\"/>\n \t\t\t<property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.H2Dialect\"/>\n\n \t\t\t<!-- 옵션 -->\n \t\t\t<property name=\"hibernate.show_sql\" value=\"true\"/>\n \t\t\t<property name=\"hibernate.format_sql\" value=\"true\"/>\n \t\t\t<property name=\"hibernate.use_sql_comments\" value=\"true\"/>\n \t\t\t<!--<property name=\"hibernate.hbm2ddl.auto\" value=\"create\" />-->\n \t\t</properties>\n\t</persistence-unit>\n</persistence> \n```\n\n첫번째로 `<persistence version=\"2.2\" ...>`태그는 사용할 JPA의 버전과 버전에 대한 Persistence.XML 문서형식 링크를 담고 있다.\n\n`<persistence-unit name=\"hello\">`태그는 하나의 지속성 단위를 의미한다. 서술적으로 표현하면, `hello`라는 이름을 가지는 `EntityManagerFactory`를 하위 설정에 따라 만들것이라는 의미이다. \n\n`<properties>`는 `<persistence-unit>`이 엑세스할 DB의 정보와 추가적인 옵션 정보들을 담는다.\n\n`<property>`는 옵션 정보를 하나씩 설정하는 부분이다.\n위 그림과 같이 `필수 속성` 부분은 엑세스할 DB에 대한 정보를 담는다.\n\n추가로 `옵션` 부분을 하나씩 살펴보자.\n`hibernate.show_sql`는 DB에 날리는 쿼리문을 확인 할것인지에 대한 설정이다.\n`hibernate.format_sql`는 보여지는 쿼리문을 포맷팅하는 설정이다.\n`hibernate.use_sql_comments`는 추가적인 `/* */` 주석을 쿼리문에 보여주는 설정이다.\n\n`hibernate.hbm2ddl.auto` 는 데이터베이스 스키마 자동생성에 대한 설정이다.\n`hibernate.hbm2ddl.auto`가 가지는 옵션은 아래와 같다.\n\n```\ncreate: 기존 테이블 삭제 후 다시 생성 (DROP + CREATE)\ncreate-drop: create와 같으나 종료시점에 테이블 DROP\nupdate: 변경된 부분만 반영 (운영 DB에 사용하면 안됌)\nvalidate: entity와 table이 정상 매핑되었는지만 확인\nnone: 사용하지 않음\n```\n\n#### **주의점❗**\n**create, create-drop, update는 운영 DB에 사용하면 안된다.  \ncreate, create-drop은 운영 DB의 데이터를 전체 삭제 시키기 때문이다.\nupdate는 아직 알아볼필요가있음**\n\n이제 설정파일을 통해 생성하는 `EntityManagerFactory`를 살펴보자.\n\n\n### EntityManagerFactory란?\n`EntityManagerFactory`는 `EntityManager`(Entity를 관리하고 DB 관련작업을 수행 함)을 생성하는 일을한다.\n\n**💡 `Entity` 는 DB 테이블에 대응하는 하나의 객체를 의미한다.**\n\n우리는 `Persistence.xml` 설정정보를 통해서 `EntityManagerFactory`를 생성할수 있다.\n\n아래의 예시코드로 살펴보자.\n``` java\npublic class JpaMain {\n\n    public static void main(String[] args) {\n        EntityManagerFactory emf = \n        \t\t\tPersistence.createEntityManagerFactory(\"hello\");\n        // 파라미터로 Persistence.xml에 설정한 persistence-unit의 name 속성값을 넣는다.\n        ...\n    }\n}\n```\n\n전 장에서 설정했던 `Persistence.xml` 파일을 통해 \n우리는 `hello`라는 이름을 가진 `<persistence-unit>`이 어떤 DB에 엑세스 할 것인지에 대한 정보를 설정했었다.\n\n위 코드가 바로 `Persistence.xml`에 작성한 Persistence Unit(지속성 단위)을 바탕으로 `EntityManagerFactory`를 생성하는 코드이다.\n\n`EntityManagerFactory`를 통해서 우리는 DB 관련 작업을 실제로 수행할 `EntityManager`를 생성 할 수 있다.\n\n#### 🔎`EntityManagerFactory`가 가지는 특징\n**1. 엔티티 매니저 팩토리는 하나의 데이터베이스에 하나만 생성한다. (생성 비용이 비싸기 때문)**\n\n**2. 엔티티 매니저 팩토리는 여러 스레드가 동시에 접근해도 안전하다.**\n\n  \n### EntityManager란?\n\n엔티티 매니저는 특정 작업을 위해 데이터베이스에 액세스 하는 역할을 한다.\n또한 엔티티를 데이터베이스에 CRUD 할 수 있다.\n즉, 엔티티와 관련된 모든 일을 처리하기에 이름 그대로 엔티티를 관리하는 관리자다.\n\n``` java\npublic class JpaMain {\n\n    public static void main(String[] args) {\n        EntityManagerFactory emf = \n        \t\t\tPersistence.createEntityManagerFactory(\"hello\");\n        // 파라미터로 Persistence.xml에 설정한 persistence-unit의 name 속성값을 넣는다.\n        \n        EntityManager em = emf.createEntityManager();\n        ...\n    }\n}\n```\n\n위와 같이 `EntityManagerFactory`를 통해서 `EntityManager`를 생성 할 수 있다.\n\n#### 🔎`EntityManager`가 가지는 특징\n**1. 엔티티 매니저는 DB connection과 밀접한 관계가 있기 때문에, 스레드 간에 공유하거나 재사용하면 안된다.**\n\n**2. 엔티티 매니저의 CRUD 작업은 트랜잭션 단위로 처리 해야한다.**\n\n#### **주의점❗**\n**EntityManager의 작업은 트랜잭션 단위로 진행되어야한다.\n왜냐하면, CRUD 작업을 수행하다가 중간에 문제가 발생하는 경우 트랜잭션 이전의 상태로 돌아가야 하기 때문이다.**\n\n\n마지막으로 코드를 통해 `EntityManagerFactory` 생성부터 DB에 엔티티의 데이터를 저장하는 과정을 살펴보자.\n\n### JPA의 구동과정\nJAVA 코드상에서 `JPA`를 통해 데이터를 DB에 저장하는 과정은 아래와 같다.\n\n```java\npublic class JpaMain {\n\n    public static void main(String[] args) {\n        EntityManagerFactory emf = \n        \t\t\tPersistence.createEntityManagerFactory(\"hello\"); \n                    // 엔티티 매니저 팩토리 생성\n        EntityManager em = emf.createEntityManager(); // 엔티티 매니저 생성\n        \n        EntityTransaction tx = em.getTransaction(); // 트랜잭션 생성\n        tx.begin(); // 트랜잭션 시작\n\n        try {\n        \tMember member = new Member() // 멤버 엔티티 생성\n            member.setId(100L)\n            member.setName(\"JPA\")\n            \n            em.persist(member) // 멤버 엔티티 데이터를 저장\n        \n            tx.commit(); // 트랜잭션 커밋\n        } catch (Exception e) {\n            tx.rollback(); // 트랜잭션 롤백\n        } finally {\n            em.close(); // 엔티티 매니저 연결 종료\n        }\n        emf.close();\n    }\n}\n```\n1. Persistence.xml 설정파일의 Persistence Unit을 통해 엔티티 매니저 팩토리를 생성한다.\n\n2. 엔티티 매니저 팩토리로 엔티티 매니저를 생성한다. \n\n3. 엔티티 매니저에 대한 트랜잭션을 생성한다.\n\n4. 트랜잭션을 시작한다.\n\n5. DB에 저장할 멤버 엔티티를 생성한다.\n\n6. em.persist(member)를 통해 엔티티 데이터를 저장한다.\n\n7. 트랜잭션을 커밋한다.\n\n8. 엔티티 매니저를 닫는다.\n\n9. 엔티티 매니저 팩토리를 닫는다.\n\n\n위와 같은 과정을 통해 DB에 데이터를 저장 할 수 있다.\n\n`em.persist()` 메서드는 파라미터로 들어온 엔티티에 대한 insert 쿼리문을 JPA가 작성하여 DB에 엔티티의 데이터를 저장하는 메서드이다.\n\n얼핏 보면, `em.persist(member)`를 실행할때 데이터가 바로 DB에 저장 될 것 같다.\n\n하지만 실제로는 `Member` 엔티티가 `EntityManager`의 영속 컨텍스트라는 공간에 저장되고 트랜잭션이 커밋되는 시점에 DB에 `Member` 엔티티에 대한 insert 쿼리문이 날라간다.\n\n왜 바로, DB에 데이터를 저장하지 않고 영속 컨텍스트에 데이터를 저장할까? 다음 포스팅에서 알아보자.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 우테코 달록팀 후디입니다. 바로 직전 포스팅으로 달록팀 백엔드의 배포 환경과 지속적 배포 환경을 구축한 방법을 소개해드렸었죠. 이번 포스팅에서는 프론트엔드의 배포 환경과 지속적 배포 환경 구성을 소개해드리려고 합니다. 바로 시작할까요? 프론트엔드 CD 다이어그램  프론트엔드의 지속적 배포 과정…","fields":{"slug":"/continuous-deploy-with-jenkins-2-frontend/"},"frontmatter":{"date":"July 24, 2022","title":"젠킨스를 사용한 달록팀의 지속적 배포 환경 구축기 (2) - 프론트엔드편","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 우테코 달록팀 후디입니다. 바로 [직전 포스팅](https://dallog.github.io/continuous-deploy-with-jenkins-1-backend/)으로 달록팀 백엔드의 배포 환경과 지속적 배포 환경을 구축한 방법을 소개해드렸었죠. 이번 포스팅에서는 프론트엔드의 배포 환경과 지속적 배포 환경 구성을 소개해드리려고 합니다. 바로 시작할까요?\n\n## 프론트엔드 CD 다이어그램\n\n![프론트엔드 지속적 배포 환경](./fe.png)\n\n프론트엔드의 지속적 배포 과정은 사실 백엔드과 크게 다른점이 없습니다. PR이 생성되고, 병합되고, 이 이벤트가 Webhook을 통해 젠킨스 서버에 전달됩니다.\n\n젠킨스 서버는 Webpack을 이용하여 리액트 프로젝트를 빌드하고, `index.html`과 `bundle.js`를 생성합니다. 달록의 프론트엔드 EC2 인스턴스에는 **NGINX가 도커 컨테이너**로 띄워져 있는데요, 생성된 정적파일은 이 NGINX 디렉토리로 전송됩니다.\n\n## 파이프라인 셋업\n\n### 사전 작업\n\n달록팀 프론트엔드 팀은 리액트를 사용합니다. 또한 달록팀은 모듈 번들러로 **웹팩(Webpack)**을 채택하여 사용하고 있습니다. 또한 웹팩을 사용하여 프로젝트를 빌드하기 위해서는 빌드 서버에 **node.js**가 설치되어있어야 합니다.\n\n이를 위해서 달록팀은 젠킨스에 `NodeJS` 플러그인을 설치했습니다. 플러그인을 설치한 이후 사용할 node.js의 버전을 선택해야합니다. **Jenkins 관리 > Global Tool Configuration > NodeJS > NodeJS Installation** 에 들어가 사용할 node.js 버전을 선택하고 이름을 지정해줍니다.\n\n달록팀은 개발 환경에서 node.js 16.14.0을 사용하므로 16.14.0 버전을 선택하고 이름을 `NodeJS 16.14.0`로 지정하였습니다.\n\n### 파이프라인 스크립트\n\n```groovy\npipeline {\n   agent any\n   stages {\n       stage('Github') {\n           steps {\n               git branch: 'develop', url: 'https://github.com/woowacourse-teams/2022-dallog.git'\n           }\n       }\n       stage('Build') {\n           steps {\n               dir('frontend') {\n                   nodejs(nodeJSInstallationName: 'NodeJS 16.14.0') {\n                        sh 'npm install && npm run build'\n                    }\n               }\n           }\n       }\n       stage('Deploy') {\n           steps {\n               dir('frontend/dist') {\n                   sshagent(credentials: ['key-dallog']) {\n                        sh 'ls'\n                        sh 'scp ./index.html ubuntu@192.168.XXX.XXX:/home/ubuntu/'\n                        sh 'scp ./bundle.js ubuntu@192.168.XXX.XXX:/home/ubuntu/'\n                        sh 'ssh ubuntu@192.168.XXX.XXX \"sudo mv ./index.html ./html && sudo mv ./bundle.js ./html\"'\n                   }\n               }\n           }\n       }\n   }\n}\n```\n\n#### Build stage\n\n`nodejs` Directive를 사용하여 사용할 NodeJS Installation의 이름을 입력하고, 실행할 명령을 입력합니다. `npm install` 로 필요한 모듈을 설치하고, `npm run build` 명령으로 빌드합니다.\n\n#### Deploy stage\n\n백엔드와 마찬가지로 `sshagent` 를 사용하여 프론트엔드 배포 서버에 SSH로 접속합니다. 빌드된 두개의 파일 `index.html`과 `bundle.js` 을 전송합니다.\n\n이후 전송된 파일을 원격지의 sudo 권한으로 `html` 디렉토리로 이동합니다. 이 `html` 디렉토리는 이후 설명하겠지만, NGINX 컨테이너 내부의 디렉토리와 마운팅된 디렉토리입니다. 배포 서버의 Docker가 sudo 권한으로 실행되고 있으므로, 파일을 해당 디렉토리로 이동시킬 때에도 sudo 권한이 필요합니다.\n\n## 도커 및 NGINX 설정\n\n### docker-compose\n\n맨 위의 다이어그램과 같이 프론트엔드 배포 서버는 NGINX가 도커로 띄워져있습니다. 이를 위한 **docker-compose.yml** 내용은 아래와 같습니다.\n\n```yaml\nversion: \"3\"\nservices:\n  dallog-front:\n    image: nginx\n    volumes:\n      - ./html:/usr/share/nginx/html\n    ports:\n      - 80:80\n```\n\n`/usr/share/nginx/html` 디렉토리는 NGINX가 서빙할 정적 파일이 위치될 디렉토리입니다. 이를 컨테이너 외부의 `./html` 디렉토리와 볼륨 마운팅 설정을 해두었습니다.\n\n### 클라이언트 사이드 라우팅 대응\n\n달록은 리액트 라우터를 사용하여 클라이언트 사이드 라우팅을 구현하였습니다. 따라서 이를 위해서 NGINX에 별도 설정이 필요합니다. `index.html` 로 접속하면 html 파일을 잘 찾을 수 있지만, 그 외의 라우트로 접속한다면 html 파일 자체를 찾지 못해 404 에러를 반환하기 때문입니다.\n\n도커 내부의 `/etc/nginx/conf.d/default.conf` 파일을 열어 아래와 같이 설정을 수정합니다.\n\n```\n...\n\nlocation / {\n\t    root   /usr/share/nginx/html;\n\t    index  index.html index.htm;\n\t    try_files $uri $uri/ /index.html;\n}\n\n...\n```\n\n알 수 없는 라우트로 접속했을 때 자동으로 `index.html` 로 Fallback 되도록 `try_files` 설정을 추가하였습니다. 파일을 수정하고 도커 컨테이너를 재시작해주면 설정이 완료됩니다.\n\n## 트러블 슈팅\n\n### 빌드 시 메모리 부족 문제\n\n리액트 프로젝트를 빌드할 때 아래와 같은 메모리 이슈를 마주하게 되었습니다.\n\n```\n<--- Last few GCs --->\n\n[21854:0x147b3960]    21931 ms: Scavenge 456.7 (469.5) -> 456.4 (470.7) MB, 3.2 / 0.0 ms  (average mu = 0.381, current mu = 0.228) allocation failure\n[21854:0x147b3960]    21939 ms: Scavenge 457.4 (470.7) -> 457.2 (475.2) MB, 4.5 / 0.0 ms  (average mu = 0.381, current mu = 0.228) allocation failure\n\n\n<--- JS stacktrace --->\n\nFATAL ERROR: Reached heap limit Allocation failed - JavaScript heap out of memory\n 1: 0xaf9c78 node::Abort() [webpack]\n 2: 0xa21a88 node::FatalError(char const*, char const*) [webpack]\n 3: 0xccdec8 v8::Utils::ReportOOMFailure(v8::internal::Isolate*, char const*, bool) [webpack]\n```\n\n**Jenkins 관리 > 시스템 설정 > Global Properties** 에서 `NODE_OPTIONS` 환경변수를 `-max-old-space-size=3072` 로 설정하여 메모리 이슈를 해결하였습니다.\n\n## 마치며\n\n현재 생성된 PR에 대한 코드를 검사하고 정상적으로 빌드되지 않은 PR의 병합을 막기 위해 Github Actions를 사용하고 있습니다. 가능하다면 조금 더 학습하여 이후 이런 작업까지 젠킨스에서 처리하도록 할 계획이 있습니다.\n\n또한 현재 백엔드와 프론트엔드 모두 `develop` 이라는 하나의 브랜치에서 작업을 하다보니 백엔드, 프론트엔드 어느 한쪽의 코드만 병합되어도 백엔드, 프론트엔드 두개의 빌드 프로세스가 실행되는 이슈가 존재합니다. 이후에는 빌드 트리거를 조금 더 세분화하여 Github의 라벨을 기반으로 트리거 되도록 개선할 계획을 가지고 있습니다.\n\n여기까지 달록팀의 젠킨스를 사용한 달록팀의 지속적 배포 환경 구축기를 읽어주셔서 감사드립니다 🙂\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 달록팀 후디입니다. 지난번 포스팅에서 달록팀이 도커를 활용하여 EC2 인스턴스에 도커를 설치한 과정을 이야기 드렸었죠. 이번 포스팅에서는 젠킨스를 활용해서 백엔드와 프론트엔드에 지속적 배포 환경을 구성한 과정에 대해 이야기 드리려고 합니다. 달록팀 지속적 배포 환경 일단 현재 구성된 달록팀의 …","fields":{"slug":"/continuous-deploy-with-jenkins-1-backend/"},"frontmatter":{"date":"July 24, 2022","title":"젠킨스를 사용한 달록팀의 지속적 배포 환경 구축기 (1) - 백엔드편","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 달록팀 후디입니다. 지난번 포스팅에서 달록팀이 도커를 활용하여 EC2 인스턴스에 도커를 설치한 과정을 이야기 드렸었죠. 이번 포스팅에서는 젠킨스를 활용해서 백엔드와 프론트엔드에 지속적 배포 환경을 구성한 과정에 대해 이야기 드리려고 합니다.\n\n## 달록팀 지속적 배포 환경\n\n일단 현재 구성된 달록팀의 지속적 배포 환경을 간단한 다이어그램으로 살펴보겠습니다.\n\n![백엔드 지속적 배포 환경](./be.png)\n\n우선, 달록의 백엔드 개발자가 열심히 기능을 개발하여 Github에 PR을 생성합니다. 이때 PR 코드가 정상적으로 빌드되고, 모든 테스트를 통과하는지 **Github Actions**를 사용하여 우선적으로 검사합니다. 이때, PR 브랜치의 코드가 문제가 있다면 develop 브랜치로 병합이 불가능합니다.\n\n> 달록팀의 브랜치 전략에 대해서는 달록팀 매트가 작성한 '[달록팀의 git 브랜치 전략을 소개합니다.](https://dallog.github.io/git-branch-strategy/)' 포스팅을 참고해주세요!\n\n정상적으로 빌드가 되는 코드는 이후 달록 개발자들끼리 코드리뷰가 진행되고, develop 브랜치에 병합이 됩니다. 이때, Github는 달록팀이 구축한 젠킨스 서버에 **Webhook**을 통해 병합 사실을 알립니다.\n\nWebhook이란, 특정한 애플리케이션이 다른 애플리케이션으로 **이벤트 발생 정보를 실시간으로 제공하기 위한 방법**입니다. 젠킨스는 외부에 Webhook URL을 열어두고, Github으로부터 이 Webhook URL로 요청을 받아 이벤트가 발생한 즉시 그 사실을 알 수 있습니다.\n\nWebhook을 통해 신호를 받은 젠킨스는 미리 지정된 젠킨스 파이프라인 스크립트를 실행하여 스프링부트 어플리케이션을 빌드하여 `jar` 파일을 생성합니다.\n\n생성한 `jar` 파일은 스프링부트 어플리케이션이 실행되고 있는 EC2 인스턴스로 전송됩니다. 그리고 스프링부트 인스턴스에서 `jar` 파일이 실행되어 배포가 완료됩니다.\n\n## 프리스타일과 파이프라인\n\n젠킨스에서 잡(Job)을 생성하는 방식은 크게 **프리스타일과 파이프라인** 두가지로 나뉩니다.\n\n### 프리스타일\n\n프리스타일은 **GUI기반**으로 젠킨스 잡을 구성할 수 있습니다. 따라서 간단한 작업에 적합하고, **복잡한 작업에는 다소 적절치 않습니다.** 또한 GUI 기반이므로 설정 과정이 다소 난잡하게 느껴질 수 있습니다.\n\n### 파이프라인\n\n반면 젠킨스 파이프라인은 일련의 배포 과정을 **코드로 작성**할 수 있습니다. 이 코드는 **Jenkinsfile** 라고 불리는 파일로 관리할 수 있어, Git 등을 통한 **버전관리**도 가능합니다. 또한 파이프라인을 사용하면 스테이지(Stage)라는 단위로 각 작업에 소요되는 시간, 실패여부를 **시각화**하여 확인할 수 있습니다.\n\n달록은 프리스타일 대신 젠킨스 파이프라인을 이용하여 잡(Job)을 생성했습니다. 개발자 입장에서는 아무래도 GUI보다는 코드 기반으로 작업 과정을 작성하는 것이 편하게 느껴집니다. 또한 추후 배포 프로세스 자체를 Github 저장소에서 관리할 수 있다는 것이 큰 장점으로 다가왔습니다.\n\n#### Scripted vs Declarative\n\n파이프라인도 문법에 따라 크게 **Scripted Pipeline**과 **Declarative Pipeline** 두가지로 나뉩니다.\n\nScripted는 Groovy라는 언어로 작성되며, 변수 선언등이 지원되어 프로그래밍을 할 수 있다는 특징이 있다. 반면, Declarative는 Scripted에 비해 간단하며, Groovy를 알지 않아도 사용할 수 있다는 장점이 존재합니다. **Scripted 문법은 Declarative에 비해 유연성이나 확장성이 높지만, 복잡도와 유지보수 난이도가 더 높습니다**.\n\n최근 CI/CD 기조는 Declarative 스타일로 많이 이동되고 있다고 합니다 ([참고](https://www.theserverside.com/answer/Declarative-vs-scripted-pipelines-Whats-the-difference)). 당장 비교적 최근에 출시된 Github Actions 도 YAML 기반의 Declarative 스타일만 사용할 수 있습니다. 따라서 달록팀은 Declarative 문법을 사용하기로 결정했습니다.\n\n## 잡(Job) 생성 및 세팅\n\n### 파이프라인으로 생성\n\n![](./pipeline.png)\n\n젠킨스 메뉴에서 '새로운 Item'을 클릭하고, 'Pipeline'을 선택하여 새로운 잡을 생성합니다.\n\n### Github URL 설정\n\n**General > GitHub project > Project url** 에 저희 깃허브 저장소 주소인 **https://github.com/woowacourse-teams/2022-dallog** 를 입력합니다.\n\n### 오래된 빌드 삭제\n\n빌드 이력을 오래 가지고 있어봤자 큰 의미가 없을 것 같기도 하고, 아무래도 **t4g.micro** 에서 돌리다보니 용량이 넉넉치 않기도 합니다. 따라서 **General > 오래된 빌드 삭제** 옵션을 클릭하여 활성화하고, 보관할 최대 갯수를 3으로 지정했습니다.\n\n> 나중에 빌드 이력 보관 개수가 너무 적다고 판단되면 조금 더 늘릴 생각입니다 🙂\n\n### 빌드 트리거 설정\n\n**Build Triggers > GitHub hook trigger for GITScm polling** 을 체크해줍니다. Github의 Webhook을 통해 빌드가 트리거되는 옵션입니다.\n\n> 이 기능은 **GitHub plugin**에서 제공하는 기능입니다. 최초 젠킨스를 설치했을 때 Install suggested plugins 를 선택하지 않으면 이 플러그인이 설치되어있지 않을수도 있습니다.\n\n#### Github 저장소에 Webhook 등록하기\n\nWebhook을 사용하려면 Github 저장소의 Settings > Webhooks 에서 Webhook URL을 등록해주어야 합니다.\n\n```\nhttp://{서비스 IP 혹은 도메인 주소}/github-webhook/\n```\n\n위와 같이 URL을 등록해줍니다. 이때 URL의 마지막에 `/`가 들어가지 않으면 오류가 발생하니 꼭 추가합니다.\n\n## 파이프라인 작성\n\n이제 빌드가 트리거 되었을 때 실행될 파이프라인 스크립트를 작성할 차례입니다.\n\n```groovy\npipeline {\n   agent any\n   stages {\n       stage('Github') {\n           steps {\n               git branch: 'develop', url: 'https://github.com/woowacourse-teams/2022-dallog.git'\n           }\n       }\n       stage('Build') {\n           steps {\n               dir('backend') {\n                   sh \"./gradlew bootJar\"\n               }\n           }\n       }\n       stage('Deploy') {\n           steps {\n               dir('backend/build/libs') {\n                   sshagent(credentials: ['key-dallog']) {\n                        sh 'scp backend-0.0.1-SNAPSHOT.jar ubuntu@192.168.XXX.XXX:/home/ubuntu'\n                        sh 'ssh ubuntu@192.168.XXX.XXX \"sh run.sh\" &'\n                   }\n               }\n           }\n       }\n   }\n}\n```\n\n### Github stage\n\nGithub stage에서는 사용할 깃허브의 저장소 주소와 브랜치를 설정합니다. 젠킨스는 이 스테이지에 명시된 저장소와 브랜치를 기준으로 코드를 가져옵니다.\n\n### Build stage\n\n`gradlew` 파일을 사용하여 빌드합니다. 이때, `dir` 지시어(Directive)를 사용하여 명령을 수행할 디렉토리를 지정할 수 있습니다.\n\n### Deploy stage\n\n> 이 작업을 위해서 사전에 Jenkins 관리 > Manage Credentials 에서 'SSH Username with private key' 로 AWS에서 발급 받은 PEM 키를 먼저 등록해야합니다.\n\n달록팀은 SSH Agent 플러그인을 사용하여 배포 서버에 원격으로 접속합니다. SSH Agent 플러그인을 사용하여 파이프라인에서 젠킨스에 등록해둔 SSH 자격증명을 쉽게 사용할 수 있습니다. 플러그인을 설치하면 `sshagent` 라는 Directive를 사용할 수 있는데, 인자로 사전에 젠킨스 Credential로 등록한 PEM키의 이름을 넣어줍니다. 이렇게 만들어진 `sshagent` Directive Block 내부에서 `sh` 를 통해 `ssh` 관련 명령을 수행할 수 있습니다.\n\n#### jar 파일 전송\n\nSCP(Secure Copy)는 SSH 통신 기반으로 원격지에 파일이나 디렉토리를 전송할 수 있는 프로토콜입니다. 리눅스에서는 `scp` 명령을 통해 SCP 프로토콜을 사용할 수 있습니다. 달록팀은 이 `scp` 명령을 통해 스프링부트 애플리케이션이 실행되고 있는 EC2 인스턴스로 빌드된 `jar` 파일을 전송합니다.\n\n> 여기서 IP주소가 프라이빗 IP로 적혀있습니다. 우테코에서 제공되는 EC2 인스턴스의 보안그룹 설정상 잠실 및 선릉 캠퍼스의 IP로만 SSH(22번 포트) 인바운드가 허용되어 있습니다. 따라서 퍼블릭 IP 대신 프라이빗 IP로 지정해주었습니다. 프라이빗 IP를 사용해도 두 EC2 인스턴스가 같은 VPC에 존재하므로 정상적으로 접근이 가능합니다.\n\n#### 원격지의 쉘 스크립트 실행\n\n스프링부트 인스턴스에는 사전에 `run.sh` 라는 이름의 쉘 스크립트가 작성되어 있습니다. 이 쉘 스크립트는 현재 실행중인 스프링부트 애플리케이션의 프로세스를 제거하고, 환경변수를 설정하고, 같은 디렉토리에 있는 `jar` 파일을 실행합니다. 스크립트는 아래와 같습니다.\n\n```shell\n#! /bin/bash\n\nPROJECT_NAME=backend\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n\nif [ -z \"$CURRENT_PID\" ]; then\n    echo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n    echo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n    kill -15 $CURRENT_PID\nfi\n\necho \"\\n🌈 SpringBoot 환경변수 설정\"\n\nexport GOOGLE_CLIENT_ID=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_CLIENT_SECRET=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_REDIRECT_URI=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_TOKEN_URI=\"XXXXXXXXXXXXXX\"\nexport JWT_EXPIRE_LENGTH=86400000\nexport JWT_SECRET_KEY=\"XXXXXXXXXXXXXX\"\n\necho \"\\n🌈 SpringBoot 애플리케이션을 실행합니다.\\n\"\n\nJAR_NAME=$(ls | grep .jar | head -n 1)\nsudo -E nohup java -jar /home/ubuntu/$JAR_NAME &\n```\n\n지난 포스팅인 **[쉘 스크립트와 함께하는 달록의 스프링부트 어플리케이션 배포 자동화](/deploy-automation-with-shell-script)** 에서 소개드린 쉘 스크립트에서 소스코드를 Pull 해오고 빌드하는 과정만 사라진 스크립트입니다.\n\n이 원격지에 있는 스크립트를 `ssh` 명령을 통해 실행하게되면 배포과정이 완료됩니다.\n\n## 트러블슈팅\n\n### Webhook 트리거 이슈\n\n처음 젠킨스를 설정할 때 아무리 Webhook 설정을 건드려봐도 빌드가 트리거되지 않는 이슈가 발생했었습니다.\n\n먼저 위 파이프라인 스크립트의 Github 스테이지에서 사용된 `git` Directive를 명시적으로 사용해줘야합니다.\n\n```\ngit branch: 'develop', url: 'https://github.com/woowacourse-teams/2022-dallog.git'\n```\n\n또한 최초로 '지금 빌드' 버튼을 클릭해야지 그 이후 Webhook 요청을 수신할 수 있게됩니다.\n\n### SSH 자격 증명 이슈\n\n```\nHost key verification failed.\nlost connection\n```\n\n젠킨스 서버에서 다른 인스턴스에 최초로 SSH 연결을 시도할 경우 젠킨스에서 위와 같은 에러가 발생할 수 있습니다. 다들 SSH로 최초 원격접속 시 아래와 같은 메시지를 본 적이 있을 것 입니다.\n\n```\nroot@XXXXXXXX:/home# ssh -i key.pem ubuntu@192.168.XXX.XXX\nThe authenticity of host '192.168.XXX.XXX (192.168.XXX.XXX)' can't be established.\nECDSA key fingerprint is SHA256:XXXXXXXXXXXXXXXX\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n```\n\n터미널에서 직접 접속할 때에는 yes 를 입력하면, 곧바로 접속될텐데요. 젠킨스 파이프라인에서는 불가능합니다. 따라서 미리 젠킨스 측의 `known_hosts` 에 원격지의 공개키를 등록해야합니다.\n\n`keyscan` 명령을 통해서 호스트의 공개키를 수집할 수 있습니다. 이 명령을 통해서 `~/.ssh/known_hosts` 에 접속할 호스트의 공개키를 추가해야합니다.\n\n## 마치며\n\n이번 포스팅으로 달록팀이 젠킨스를 활용하여 스프링부트 애플리케이션의 지속적 배포 환경을 구축한 방법을 소개드렸습니다. 이어지는 다음 포스팅에서는 달록팀의 프론트엔드 배포 환경과 지속적 배포 환경 구축 방법을 소개드리려고 합니다. 읽어주셔서 감사드립니다 😄\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. 외부와 의존성 분리하기 도메인 로직은 우리가 지켜야할 매우 소중한 비즈니스 로직들이 담겨있다. 이러한 도메인 로직들은 변경이 최소화되어야 한다. 그렇기 때문에 외부와의 의존성을 최소화 해야 한다.  인터페이스 활용하기 우선 우리가 지금까지 학습한 것 중 객체 간의 의존성을 약하게 만들어 줄 수 있는 수…","fields":{"slug":"/separated-interface/"},"frontmatter":{"date":"July 24, 2022","title":"외부와 의존성 분리하기","tags":["분리된 인터페이스","의존성 분리"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## 외부와 의존성 분리하기\n\n도메인 로직은 우리가 지켜야할 매우 소중한 비즈니스 로직들이 담겨있다. 이러한 도메인 로직들은 변경이 최소화되어야 한다. 그렇기 때문에 외부와의 의존성을 최소화 해야 한다. \n\n### 인터페이스 활용하기\n\n우선 우리가 지금까지 학습한 것 중 객체 간의 의존성을 약하게 만들어 줄 수 있는 수단으로 인터페이스를 활용할 수 있다. 간단한 예시로 `JpaRepository`를 살펴보자.\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n\n    Optional<Member> findByEmail(final String email);\n\n    boolean existsByEmail(final String email);\n}\n```\n\n이러한 인터페이스 덕분에 우리는 실제 DB에 접근하는 내부 구현에 의존하지 않고 데이터를 조작할 수 있다. 핵심은 `실제 DB에 접근하는 행위`이다.\n\n아래는 `Spring Data`가 만든 `JpaRepository의 구현체` `SimpleJpaRepository`의 일부를 가져온 것이다.\n\n```java\n@Repository\n@Transactional(readOnly = true)\npublic class SimpleJpaRepository<T, ID> implements JpaRepositoryImplementation<T, ID> {\n\n\tprivate static final String ID_MUST_NOT_BE_NULL = \"The given id must not be null!\";\n\n\tprivate final JpaEntityInformation<T, ?> entityInformation;\n\tprivate final EntityManager em;\n\tprivate final PersistenceProvider provider;\n\n\tprivate @Nullable CrudMethodMetadata metadata;\n\tprivate EscapeCharacter escapeCharacter = EscapeCharacter.DEFAULT;\n\n\tpublic SimpleJpaRepository(JpaEntityInformation<T, ?> entityInformation, EntityManager entityManager) {\n\n\t\tAssert.notNull(entityInformation, \"JpaEntityInformation must not be null!\");\n\t\tAssert.notNull(entityManager, \"EntityManager must not be null!\");\n\n\t\tthis.entityInformation = entityInformation;\n\t\tthis.em = entityManager;\n\t\tthis.provider = PersistenceProvider.fromEntityManager(entityManager);\n\t}\n  ...\n}\n```\n\n해당 구현체는 `entityManger`를 통해 객체를 영속 시키는 행위를 진행하고 있기 때문에 `영속 계층`에 가깝다고 판단했다. 즉 도메인의 입장에서 `MemberRepository`를 바라볼 때 단순히 `JpaRepository`를 상속한 인터페이스를 가지고 있기 때문에 `영속 계층`에 대한 직접적인 의존성은 없다고 봐도 무방하다. 정리하면 우리는 인터페이스를 통해 실제 구현체에 의존하지 않고 로직을 수행할 수 있게 된다. \n\n### 관점 변경하기\n\n이러한 사례를 외부 서버와 통신을 담당하는 우리가 직접 만든 인터페이스인 `OAuthClient`에 대입해본다. `OAuthClient`의 가장 큰 역할은 n의 소셜에서 `OAuth 2.0`을 활용한 인증의 행위를 정의한 인터페이스이다. google, github 등 각자에 맞는 요청을 처리하기 위해 `OAuthClient`를 구현한 뒤 로직을 처리할 수 있다. 아래는 실제 google의 인가 코드를 기반으로 토큰 정보에서 회원 정보를 조회하는 로직을 담고 있다.\n\n```java\npublic interface OAuthClient {\n\n    OAuthMember getOAuthMember(final String code);\n}\n```\n\n```java\n@Component\npublic class GoogleOAuthClient implements OAuthClient {\n\n    private static final String JWT_DELIMITER = \"\\\\.\";\n\n    private final String googleRedirectUri;\n    private final String googleClientId;\n    private final String googleClientSecret;\n    private final String googleTokenUri;\n    private final RestTemplate restTemplate;\n    private final ObjectMapper objectMapper;\n\n    public GoogleOAuthClient(@Value(\"${oauth.google.redirect_uri}\") final String googleRedirectUri,\n                             @Value(\"${oauth.google.client_id}\") final String googleClientId,\n                             @Value(\"${oauth.google.client_secret}\") final String googleClientSecret,\n                             @Value(\"${oauth.google.token_uri}\") final String googleTokenUri,\n                             final RestTemplate restTemplate, final ObjectMapper objectMapper) {\n        this.googleRedirectUri = googleRedirectUri;\n        this.googleClientId = googleClientId;\n        this.googleClientSecret = googleClientSecret;\n        this.googleTokenUri = googleTokenUri;\n        this.restTemplate = restTemplate;\n        this.objectMapper = objectMapper;\n    }\n\n    @Override\n    public OAuthMember getOAuthMember(final String code) {\n        GoogleTokenResponse googleTokenResponse = requestGoogleToken(code);\n        String payload = getPayloadFrom(googleTokenResponse.getIdToken());\n        String decodedPayload = decodeJwtPayload(payload);\n\n        try {\n            return generateOAuthMemberBy(decodedPayload);\n        } catch (JsonProcessingException e) {\n            throw new IllegalArgumentException();\n        }\n    }\n\n    private GoogleTokenResponse requestGoogleToken(final String code) {\n        HttpHeaders headers = new HttpHeaders();\n        headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED);\n        MultiValueMap<String, String> params = generateRequestParams(code);\n\n        HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(params, headers);\n        return restTemplate.postForEntity(googleTokenUri, request, GoogleTokenResponse.class).getBody();\n    }\n\n    private MultiValueMap<String, String> generateRequestParams(final String code) {\n        MultiValueMap<String, String> params = new LinkedMultiValueMap<>();\n        params.add(\"client_id\", googleClientId);\n        params.add(\"client_secret\", googleClientSecret);\n        params.add(\"code\", code);\n        params.add(\"grant_type\", \"authorization_code\");\n        params.add(\"redirect_uri\", googleRedirectUri);\n        return params;\n    }\n\n    private String getPayloadFrom(final String jwt) {\n        return jwt.split(JWT_DELIMITER)[1];\n    }\n\n    private String decodeJwtPayload(final String payload) {\n        return new String(Base64.getUrlDecoder().decode(payload), StandardCharsets.UTF_8);\n    }\n\n    private OAuthMember generateOAuthMemberBy(final String decodedIdToken) throws JsonProcessingException {\n        Map<String, String> userInfo = objectMapper.readValue(decodedIdToken, HashMap.class);\n        String email = userInfo.get(\"email\");\n        String displayName = userInfo.get(\"name\");\n        String profileImageUrl = userInfo.get(\"picture\");\n\n        return new OAuthMember(email, displayName, profileImageUrl);\n    }\n}\n```\n\n보통의 생각은 인터페이스인 `OAuthClient`와 구현체인 `GoogleOAuthClient`를 같은 패키지에 두려고 할 것이다. `GoogleOAuthClient`는 외부 의존성을 강하게 가지고 있기 때문에 `domain` 패키지와 별도로 관리하기 위한 `infrastructure` 패키지가 적합할 것이다. 결국 인터페이스인 `OAuthClient` 또한 `infrastructure`에 위치하게 될 것이다. 우리는 이러한 생각에서 벗어나 새로운 관점에서 살펴봐야 한다.\n\n앞서 언급한 의존성에 대해 생각해보자. 위 `OAuthClient`를 사용하는 주체는 누구일까? 우리는 이러한 주체를 `domain` 내에 인증을 담당하는 `auth` 패키지 내부의 `Authservice`로 결정 했다. 아래는 실제 `OAuthClient`를 사용하고 있는 주체인 `AuthService`이다.\n\n```java\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\n    private final OAuthEndpoint oAuthEndpoint;\n    private final OAuthClient oAuthClient;\n    private final MemberService memberService;\n    private final JwtTokenProvider jwtTokenProvider;\n\n    public AuthService(final OAuthEndpoint oAuthEndpoint, final OAuthClient oAuthClient,\n                       final MemberService memberService, final JwtTokenProvider jwtTokenProvider) {\n        this.oAuthEndpoint = oAuthEndpoint;\n        this.oAuthClient = oAuthClient;\n        this.memberService = memberService;\n        this.jwtTokenProvider = jwtTokenProvider;\n    }\n\n    public String generateGoogleLink() {\n        return oAuthEndpoint.generate();\n    }\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        String email = oAuthMember.getEmail();\n\n        if (!memberService.existsByEmail(email)) {\n            memberService.save(generateMemberBy(oAuthMember));\n        }\n\n        Member foundMember = memberService.findByEmail(email);\n        String accessToken = jwtTokenProvider.createToken(String.valueOf(foundMember.getId()));\n\n        return new TokenResponse(accessToken);\n    }\n\n    private Member generateMemberBy(final OAuthMember oAuthMember) {\n        return new Member(oAuthMember.getEmail(), oAuthMember.getProfileImageUrl(), oAuthMember.getDisplayName(), SocialType.GOOGLE);\n    }\n}\n```\n\n지금 까지 설명한 구조의 패키지 구조는 아래와 같다.\n\n```\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   └── application\n    │   │               │       └── AuthService.java\n    │   │               ...\n    │   │               ├── infrastructure\n    │   │               │   ├── oauth\n    │   │               │   │   └── client\n    │   │               │   │       ├── OAuthClient.java\n    │   │               │   │       └── GoogleOAuthClient.java\n    │   │               │   └── dto\n    │   │               │       └── OAuthMember.java     \n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n결국 이러한 구조는 아래와 같이 `domain` 패키지에서 `infrastructure`에 의존하게 된다.\n  \n```java\n...\nimport com.allog.dallog.infrastructure.dto.OAuthMember; // 의존성 발생!\nimport com.allog.dallog.infrastructure.oauth.client.OAuthClient; // 의존성 발생!\n...\n\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\t...\n    private final OAuthClient oAuthClient;\n    ...\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        ...\n    }\n    ...\n}\n```\n\n### Separated Interface Pattern\n\n`분리된 인터페이스`를 활용하자. 즉 `인터페이스`와 `구현체`를 각각의 패키지로 분리한다. 분리된 인터페이스를 사용하여 `domain` 패키지에서 인터페이스를 정의하고 `infrastructure` 패키지에 구현체를 둔다. 이렇게 구성하면 인터페이스에 대한 종속성을 가진 주체가 구현체에 대해 인식하지 못하게 만들 수 있다.\n\n아래와 같은 구조로 인터페이스와 구현체를 분리했다고 가정한다.\n\n```\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   ├── application\n    │   │               │   │   ├── AuthService.java\n    │   │               │   │   └── OAuthClient.java\n    │   │               │   └── dto\n    │   │               │       └── OAuthMember.java         \n    │   │               ...\n    │   │               ├── infrastructure\n    │   │               │   ├── oauth\n    │   │               │       └── client\n    │   │               │           └── GoogleOAuthClient.java\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n자연스럽게 `domain` 내에 있던 `infrastructure` 패키지에 대한 의존성도 제거된다. 즉 외부 서버와의 통신을 위한 의존성이 완전히 분리된 것을 확인할 수 있다.\n\n```java\n...\nimport com.allog.dallog.auth.dto.OAuthMember; // auth 패키지 내부를 의존\n...\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\t...\n    private final OAuthClient oAuthClient;\n    ...\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        ...\n    }\n    ...\n}\n```\n\n## References.\n\n[Separated Interface](https://www.martinfowler.com/eaaCatalog/separatedInterface.htmlhttps://www.martinfowler.com/eaaCatalog/separatedInterface.html)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 우테코 달록팀 후디입니다. 이번 스프린트에서는 저는 배포와 CI/CD와 같이 인프라와 관련된 태스크에 집중하고 있습니다. 지난번 포스팅으로 달록팀이 쉘 스크립트를 통해 배포 자동화를 구축한 이야기를 했었죠. 하지만 새로운 기능이 병합될 때 마다 SSH로 EC2 인스턴스에 접속하여 쉘 스크립트를…","fields":{"slug":"/install-jenkins-with-docker-on-ec2/"},"frontmatter":{"date":"July 21, 2022","title":"EC2 환경에서 도커를 활용한 젠킨스 설치하기","tags":["DevOps","Jenkins"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 우테코 달록팀 후디입니다. 이번 스프린트에서는 저는 배포와 CI/CD와 같이 인프라와 관련된 태스크에 집중하고 있습니다.\n\n지난번 포스팅으로 달록팀이 쉘 스크립트를 통해 배포 자동화를 구축한 이야기를 했었죠. 하지만 새로운 기능이 병합될 때 마다 SSH로 EC2 인스턴스에 접속하여 쉘 스크립트를 **매번 실행해야한다는 단점**이 존재했습니다. 따라서 저희 팀은 메인 브랜치에 기능이 새로 병합 될 때마다 자동으로 감지하고, 스프링 어플리케이션을 `jar` 파일로 빌드하여 배포하는 환경이 필요하다고 느꼈습니다. 따라서 **CI/CD 도구를 도입**하기로 결정했습니다.\n\n이번 포스팅에서는 달록이 EC2 환경에서 도커를 사용하여 젠킨스를 설치한 방법에 대해서 정리합니다.\n\n## 젠킨스 도입 배경\n\n![유명한 CI/CD 도구들의 구글 트렌드 결과](./google-trends.png)\n\n달록은 CI/CD 도구로 **Jenkins**를 선정하였습니다. 위 사진은 시중에 배포되어있는 여러 CI/CD 도구의 구글 트렌드 분석 결과입니다. 파란색이 Jenkins 인데요, **압도적으로 높은 관심도**를 유지하고 있습니다.\n\n아무래도 달록팀 모두가 CI/CD에 익숙하지 않아 가장 사람들이 많이 사용하고, 그에 따라 **생태계가 넓고 레퍼런스가 많은 도구**를 선정하는 것이 좋다고 판단하였습니다. **레퍼런스가 많아** 초기 학습 비용이 적게 들고, 이슈가 발생했을때에도 **트러블슈팅이 비교적 쉽다**고 생각했습니다.\n\n## 도커\n\n달록팀은 EC2 인스턴스에 **도커를 사용하여** 젠킨스 컨테이너를 띄웠습니다. 도커를 사용하지 않고 젠킨스를 우분투에 직접 설치한다면 해주어야할 환경 설정이 가득합니다. 젠킨스를 돌리기 위한 JDK 설치, 젠킨스 설치, 젠킨스 포트 설정, 방화벽 설정 등등...\n\n하지만 도커를 사용하면 이런 **환경 설정 없이 간단한 명령어 몇가지로 젠킨스를 설치하고 서버에 띄울 수 있습니다.**\n\n도커는 서비스를 운용하는데 필요한 실행환경, 라이브러리, 소프트웨어, 코드 등을 컨테이너라는 단위로 가상화하는 컨테이너 기반 가상화 플랫폼입니다. 도커를 사용하면 EC2 인스턴스에는 미리 **도커라이징(Dockerizing)**된 이미지를 다운로드 받고 도커를 통해 실행하기만 하면되며, 해당 컨테이너가 어떤 환경을 필요로 하는지 전혀 알 필요가 없습니다.\n\n> 더 자세한 내용은 제가 작성한 [이론과 실습을 통해 이해하는 Docker 기초](https://hudi.blog/about-docker/)를 읽어보시면 좋을 것 같습니다 🤭\n\n### 우분투에 도커 설치\n\n> 달록팀은 EC2 t4g.micro 인스턴스에 우분투 22.04 (ARM 64) 환경을 사용하고 있습니다.\n\n> 아래 설치 방법은 [도커 공식 도큐먼트](https://docs.docker.com/engine/install/ubuntu/)에서 제공되는 내용입니다.\n\n#### 레포지토리 셋업\n\n아래 명령을 통해서 우분투의 `apt`의 패키지 인덱스를 최신화하고, `apt`가 HTTPS를 통해 패키지를 설치할 수 있도록 설정합니다.\n\n```shell\n$ sudo apt-get update\n$ sudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n```\n\n#### 도커의 공식 GPG 키 추가\n\n```shell\n$ sudo mkdir -p /etc/apt/keyrings\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n```\n\n#### 레포지토리 셋업\n\n```shell\n$ echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n```\n\n#### 도커 엔진 설치\n\n아래 명령을 실행하면 가장 최신버전의 도커 엔진이 설치됩니다.\n\n```shell\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n```\n\n#### 도커 설치 확인\n\n```shell\n$ sudo docker run hello-world\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n...\n```\n\n위 명령을 실행하여 위와 같이 `Hello from Docker!` 메시지가 출력되면 성공적으로 도커 설치가 완료된 것 입니다. 다음 단계로 넘어가볼까요? 🤗\n\n## 젠킨스 컨테이너 실행\n\n### 젠킨스 이미지 다운로드\n\n```shell\n$ docker pull jenkins/jenkins:lts\n```\n\n위 명령을 통해 Jenkins LTS(Long Term Support) 버전의 이미지를 다운로드 받습니다.\n\n### 젠킨스 컨테이너 띄우기\n\n```shell\n$ sudo docker run -d -p 8080:8080 -v /jenkins:/var/jenkins_home --name jenkins -u root jenkins/jenkins:lts\n```\n\n위 명령을 통해 다운로드 받은 젠킨스 이미지를 컨테이너로 띄울 수 있습니다. 사용된 각 옵션을 간단히 알아볼까요?\n\n- **-d** : 컨테이너를 **데몬**으로 띄웁니다.\n- **-p 8080:8080** : 컨테이너 외부와 내부 포트를 **포워딩**합니다. 좌측이 호스트 포트, 우측이 컨테이너 포트입니다.\n- **-v /jenkins:/var/jenkins_home** : 도커 컨테이너의 데이터는 **컨테이너가 종료되면 휘발**됩니다. 도커 컨테이너의 데이터를 보존하기 위한 여러 방법이 존재하는데, 그 중 한 방법이 **볼륨 마운트**입니다. 이 옵션을 사용하여 젠킨스 컨테이너의 `/var/jenkins_home` 이라는 디렉토리를 호스트의 `/jenkins` 와 마운트하고 데이터를 보존할 수 있습니다.\n- **--name jenkins** : 도커 컨테이너의 이름을 설정합니다.\n- **-u root** : 컨테이너가 실행될 리눅스의 사용자 계정을 root 로 명시합니다.\n\n### docker-compose 사용하기\n\n하지만 위와 같은 명령어를 모두 외우고 있다가, 도커 컨테이너를 실행할 때 마다 입력하게 된다면 굉장히 번거롭겠죠. 따라서 도커는 docker-compose 라는 것을 지원합니다. 도커 컴포즈는 여러 컨테이너의 실행을 한번에 관리할 수 있게 도와주는 도커의 도구입니다. 하지만 저희와 같이 하나의 컨테이너만 필요한 상황에서도 유용하게 사용할 수 있죠.\n\n```shell\n$ sudo apt install docker-compose\n```\n\n위 명령을 이용하여 `docker-compose` 를 설치합니다.\n\n그리고 도커를 실행할 경로에 `docker-compose.yml` 이란 파일을 만들고, 아래의 내용을 작성해줍니다.\n\n```yaml\nversion: \"3\"\nservices:\n  jenkins:\n    image: jenkins/jenkins:lts\n    user: root\n    volumes:\n      - ./jenkins:/var/jenkins_home\n    ports:\n      - 8080:8080\n```\n\n생성한 `docker-compose.yml` 이 존재하는 경로에서 아래의 명령을 실행하면 복잡한 명령 없이도 도커 컨테이너를 실행할 수 있습니다.\n\n```shell\n$ sudo docker-compose up -d\n```\n\n> -d 옵션은 컨테이너가 데몬으로 실행됨을 의미합니다.\n\n## 젠킨스 설정\n\n도커를 사용하여 젠킨스 컨테이너가 EC2 인스턴스에 성공적으로 띄워졌다면, EC2의 퍼블릭 IP를 통해 외부에서 접속할 수 있을 것 입니다. localhost:8080으로 접속하면 아래와 같은 화면이 보일 것 입니다.\n\n![](./unlock-jenkins.png)\n\n```shell\n$ sudo docker logs jenkins\n```\n\n위 명령을 사용하면, `jenkins` 컨테이너에 출력된 로그를 확인할 수 있습니다. 젠킨스를 최초로 설치하고 실행하면 사진에서 요구하는 initial admin password를 출력해주는데요, 로그를 확인해봅시다.\n\n```\n*************************************************************\n*************************************************************\n*************************************************************\n\nJenkins initial setup is required. An admin user has been created and a password generated.\n\nPlease use the following password to proceed to installation:\n\n\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n\nThis may also be found at: /var/jenkins_home/secrets/initialAdminPassword\n\n*************************************************************\n*************************************************************\n*************************************************************\n```\n\n위에서 표시된 `XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` 를 웹사이트에 넣어주시면 됩니다.\n\n혹은 아래의 명령으로 `jenkins` 컨테이너 내부에 직접 접속하여, `/var/jenkins_home/secrets/initialAdminPassword` 파일의 내용을 조회하는 방법도 있습니다.\n\n```shell\n$ sudo docker exec -it jenkins /bin/bash\n$ cat /var/jenkins_home/secrets/initialAdminPassword\n```\n\n![](./customize-jenkins.png)\n\n그 다음 위 화면에서 Install suggested plugins를 클릭하여 추천되는 플러그인을 설치합니다. 그 이후 요구되는 여러 정보의 입력을 끝 마쳐주세요.\n\n```\nSystem.setProperty('org.apache.commons.jelly.tags.fmt.timeZone\n```\n\n이후 대시보드에서 **Jenkins 관리 > Script Console** 에서 위의 스크립트를 입력하여 타임존을 서울로 설정하기만 하면, 젠킨스 기본 설정이 완료됩니다! 🎉\n\n## 마치며\n\n다음 포스팅에서는 달록이 젠킨스를 이용하여 어떻게 배포 자동화 프로세스를 구축하였는지 작성해보도록 하겠습니다. 많은 기대 부탁드립니다 🙏\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 웹서비스 개발팀은 새롭게 개발한 서비스의 기능을 어떻게 사용자에게 전달할까요? 새로운 기능이 메인 브랜치에 병합될 때 마다 EC2 인스턴스에 접속하여 브랜치를 Pull 하고, 프로젝트를 빌드하고, 현재 동작중인 어플리케이션의 프로세스를 종료하고, 새롭게 빌드된 어플리케이션의 프로세스를 띄우는 과정... 배…","fields":{"slug":"/deploy-automation-with-shell-script/"},"frontmatter":{"date":"July 19, 2022","title":"쉘 스크립트와 함께하는 달록의 스프링부트 어플리케이션 배포 자동화","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n웹서비스 개발팀은 새롭게 개발한 서비스의 기능을 어떻게 사용자에게 전달할까요? 새로운 기능이 메인 브랜치에 병합될 때 마다 EC2 인스턴스에 접속하여 브랜치를 Pull 하고, 프로젝트를 빌드하고, 현재 동작중인 어플리케이션의 프로세스를 종료하고, 새롭게 빌드된 어플리케이션의 프로세스를 띄우는 과정...\n\n배포가 필요할때마다 이런 명령을 수동으로 일일히 입력한다면, 그건 너무 지루한 작업아닐까요? 😫 실수라도 하면 어쩌죠? 😰\n\n우테코 달록팀 백엔드는 이런 한계점을 극복하고자 쉘 스크립트를 활용하여 배포 프로세스를 자동화하였습니다. 달록팀은 어떻게 스프링부트 어플리케이션의 배포를 자동화했을까요?\n\n## 쉘 스크립트\n\n쉘 스크립트는 유닉스/리눅스 기반 운영체제에서의 일련의 명령으로 구성된 실행가능한 텍스트 파일입니다. 원래라면 일일히 키보드로 입력해야하는 리눅스 명령을 하나의 파일에 모아두고, 한번에 실행할 수 있죠. 작성된 명령은 셸이라고 불리는 명령줄 인터프리터에서 실행되며, 위에서부터 아래로 차례로 실행됩니다. 이 쉘 스크립트를 이용해 리눅스 환경에서 여러 프로세스를 자동화할 수 있습니다.\n\n달록이 스프링부트 어플리케이션을 배포하는 환경은 Ubuntu 22 버전이므로 쉘 스크립트를 활용할 수 있습니다.\n\n## 수동으로 배포하기\n\n배포 프로세스를 자동화하려면 우선 수동으로 어떤 명령을 사용해서 배포를 하는지 알아야합니다. 어떤과정을 거쳐 배포되는지부터 알아볼까요?\n\n> 레포지토리는 이미 Clone 되어있다고 가정합니다.\n\n### 1. Git Pull\n\n```shell\n$ cd 2022-dallog/backend\n$ git pull\n```\n\n우선 Github에서 가장 최신 버전을 pull 해와야겠죠?\n\n### 2. 빌드\n\n```shell\n./gradlew bootJar\n```\n\n`gradlew` 를 사용하여 자바 프로젝트를 빌드해서 `.jar` 파일을 생성합니다.\n\n### 3. 프로세스 종료\n\n```shell\n$ ps -ef | grep jar\n$ kill -15 XXXXX\n```\n\n`ps` 명령을 사용해서 실행중인 스프링부트 어플리케이션의 PID를 알아내고, `kill` 명령을 통해 프로세스를 종료합니다.\n\n### 4. 환경변수 설정\n\n달록의 스프링부트 어플리케이션은 여러 민감한 정보를 환경변수를 사용하여 외부에 노출되지 않도록 하였습니다. 따라서 어플리케이션이 실행될 때 환경변수도 함께 설정을 해주어야합니다.\n\n```shell\n$ export GOOGLE_CLIENT_ID=\"XXXXX\"\n$ export GOOGLE_CLIENT_SECRET=\"XXXXX\"\n$ export GOOGLE_REDIRECT_URI=\"XXXXX\"\n$ export GOOGLE_TOKEN_URI=\"XXXXX\"\n$ export JWT_SECRET_KEY=\"XXXXX\"\n$ export JWT_EXPIRE_LENGTH=3600\n\n...\n```\n\n### 5. 드디어 실행\n\n```shell\n$ sudo -E nohup java -jar ./build/libs/backend-0.0.1-SNAPSHOT.jar\n```\n\n드디어 스프링부트 어플리케이션을 실행합니다.\n\n이 귀찮은 과정을 배포 할때마다 해야한다니 벌써 머리가 어질어질 하네요. 😵‍💫 그렇다면 앞서 소개드린 쉘 스크립트를 통해서 이 과정을 자동화해볼까요?\n\n## 달록의 배포 쉘 스크립트\n\n```shell\n#! /bin/bash\n\nPROJECT_PATH=/home/ubuntu/2022-dallog\nPROJECT_NAME=backend\nPROJECT_BUILD_PATH=backend/build/libs\n\ncd $PROJECT_PATH/$PROJECT_NAME\n\nclear\n\necho \"🌈 Github에서 프로젝트를 Pull 합니다.\\n\"\n\ngit pull\n\necho \"\\n🌈 SpringBoot 프로젝트 빌드를 시작합니다.\\n\"\n\n./gradlew bootJar\n\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n\nif [ -z \"$CURRENT_PID\" ]; then\n\techo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n\techo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n\tkill -15 $CURRENT_PID\nfi\n\necho \"\\n🌈 SpringBoot 환경변수 설정\"\n\nexport GOOGLE_CLIENT_ID=\"XXXXX\"\nexport GOOGLE_CLIENT_SECRET=\"XXXXX\"\nexport GOOGLE_REDIRECT_URI=\"XXXXX\"\nexport GOOGLE_TOKEN_URI=\"XXXXX\"\nexport JWT_SECRET_KEY=\"XXXXX\"\nexport JWT_EXPIRE_LENGTH=3600\n\necho \"\\n🌈 SpringBoot 애플리케이션을 실행합니다.\\n\"\n\nJAR_PATH=$(ls $PROJECT_PATH/$PROJECT_BUILD_PATH/ | grep .jar | head -n 1)\nsudo -E nohup java -jar $PROJECT_PATH/$PROJECT_BUILD_PATH/$JAR_PATH &\n```\n\n달록이 작성한 배포 자동화 쉘 스크립트는 아래와 같습니다. 차근차근 알아볼까요?\n\n### #! /bin/bash\n\n`#! /bin/bash` 은 해당 쉘 스크립트가 많은 쉘 중 **Bash Shell 로 실행됨**을 알립니다.\n\n### 변수 사용\n\n`PROJECT_PATH`, `PROJECT_NAME` 과 같이 자주 사용되는 데이터는 쉘 스크립트에서 제공하는 변수 기능으로 분리하였습니다. 이때 주의할 점은 쉘 스크립트에서 변수를 선언할 때 `=` **앞뒤에 공백이 와서는 안된다는 점** 입니다.\n\n### echo\n\n`echo` 명령을 통해 배포 프로세스가 어디까지 진행됐는지 사용자에게 알려줍니다.\n\n### 실행중인 어플리케이션의 PID 가져오기\n\n```shell\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n```\n\n#### pgrep\n\n쉘 스크립트 중 위와 같은 코드가 있었습니다. 위 코드는 우선 `pgrep` 이라는 명령을 통해서 실행중인 프로세스의 이름으로 PID 목록을 가져옵니다.\n\n#### pipe와 head\n\n그리고 파이프(`|`)명령으로 다른 프로세스로 PID 목록을 보냅니다. PID 목록은 `head` 명령으로 전달되며, `head` 명령은 PID 목록의 첫번째만을 가져옵니다.\n\n#### 명령의 실행결과를 변수에 담기\n\n이렇게 가져온 PID는 `$()` 문법을 통해 `CURRENT_PID` 변수에 저장됩니다. `$()` 는 `$(command)` 형태로 사용되며, 괄호 내부의 실행 결과를 변수로 저장하기 위해 사용됩니다.\n\n### 조건문\n\n쉘 스크립트에도 `if` 문을 사용하여 조건문을 작성할 수 있습니다. 다만, 우리에게 익숙한 프로그래밍 언어에서의 if문과는 조금 괴리가 존재해서 별도로 학습이 필요할수도 있습니다.\n\n```shell\nif [ -z \"$CURRENT_PID\" ]; then\n\techo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n\techo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n\tkill -15 $CURRENT_PID\nfi\n```\n\n위 코드는 아까 PID를 담은 `CURRENT_PID` 가 비어있는지 확인한 후 존재하지 않다면 메시지만 출력하고, 존재한다면 해당 PID를 `kill` 명령으로 종료합니다.\n\n쉘 스크립트의 `if` 문에서 `-z` 는 조건식의 종류 중 하나이며, 주어진 문자열의 길이가 0이라면 True를 나타냅니다. 확실히 조금 낯설죠? 😅\n\n### JAR파일 경로 가져오기\n\n```shell\nJAR_PATH=$(ls $PROJECT_PATH/$PROJECT_BUILD_PATH/ | grep .jar | head -n 1)\n```\n\n`ls` 명령을 통해 빌드 디렉토리의 파일 목록을 가져오고, `grep` 명령을 통해 `.jar` 파일만을 가져옵니다. 그다음 `head` 명령을 통해 단 하나의 파일만을 가져온 다음, `JAR_PATH` 변수에 저장합니다.\n\n### 어플리케이션 실행하기\n\n```shell\nsudo -E nohup java -jar $PROJECT_PATH/$PROJECT_BUILD_PATH/$JAR_PATH &\n```\n\n어플리케이션을 실행합니다.\n\n#### sudo -E\n\n`sudo` 명령 뒤에 붙은 `-E` 옵션은 유저가 설정한 환경변수를 `sudo` 명령에서도 공유하여 사용할 수 있도록 만드는 옵션입니다.\n\n#### nohup\n\n`nohup` 명령은 현재 **터미널 세션이 끊어져도 프로세스가 계속 살아있도록** 만들기 위해 사용되는 명령입니다.\n\n#### Background 프로세스\n\n그리고 명령 맨 뒤에 `&` 가 붙어있는데, 프로세스를 Foreground가 아닌 **Background에서 실행**하기 위해 붙여줍니다.\n\n## 한계점\n\n하지만, 이런 방식도 결국 한계점이 존재합니다. 특히나 달록과 같이 애자일한 조직에서는 최대한 작은 기능단위로 개발이 병렬적으로 진행되어, 메인 브랜치에 머지됩니다. 하루에 몇번이고 배포를 해야하는 상황이 발생할수도 있죠.\n그렇지 않아도 모든 개발자가 바쁘게 새로운 기능을 개발하기 바쁜데, 메인 브랜치에 병합된 시점마다 EC2 인스턴스에 접속해서 쉘 스크립트를 실행해야할까요?\n\n이런 한계점을 극복하고자, 달록팀은 앞으로 CI/CD 도구를 도입할 예정입니다. 세상에는 참 다양한 CI/CD 도구가 존재합니다. Jenkins, Github Actions, Travis CI, Circle CI, Gitlab CI/CD 등등...\n\n달록의 이번 스프린트의 배포 태스크에서는 이런 다양한 CI/CD 도구들의 장단을 분석하고 도입할 예정입니다. 많은 기대 부탁드립니다. 👏👏\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. auditing이란 엔티티와 관련된 이벤트(insert, update, delete)를 추적하고 기록하는 것을 의미한다. 모든 엔티티에 생성일시, 수정일시, 생성한 사람을 추가하고 싶은 경우를 생각해보자. 모든 엔티티에 생성일시, 수정일시, 생성한 사람에 대한 필드를 일일이 구현해주어야 한다. 이렇게 되면…","fields":{"slug":"/data-jpa-auditing/"},"frontmatter":{"date":"July 18, 2022","title":"Spring Data JPA의 Auditing","tags":["Spring","Data JPA","Auditing"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n> auditing이란 엔티티와 관련된 이벤트(insert, update, delete)를 추적하고 기록하는 것을 의미한다.\n\n모든 엔티티에 생성일시, 수정일시, 생성한 사람을 추가하고 싶은 경우를 생각해보자. 모든 엔티티에 생성일시, 수정일시, 생성한 사람에 대한 필드를 일일이 구현해주어야 한다. 이렇게 되면 모든 엔티티에 중복이 생기고 유지보수가 어려워진다. Spring Data JPA가 제공하는 `Auditing` 기능을 사용하면 이런 기능을 쉽고 빠르게 구현할 수 있다.\n\n## Spring Data JPA Auditing 적용하기\n\n`Auditing`을 적용하기 위해서는 우선 어노테이션을 적용해야 한다. `@Configuration` 어노테이션이 적용된 Config 클래스에 아래와 같이 `@EnableJpaAuditing`을 추가한다.\n\n```java\n@Configuration\n@EnableJpaAuditing\npublic class JpaConfig {\n}\n```\n\n⚠️ 주의 : 원활한 Slice 테스트를 위해 @SpringBootApplication 과 @EnableJpaAuditing 어노테이션 분리하기\n\n[https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.testing.spring-boot-applications.user-configuration-and-slicing](https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.testing.spring-boot-applications.user-configuration-and-slicing)\n\n### Spring Entity Callback Listener 적용하기\n\nAuditing entity listener class로 지정하기 위해 `@EntityListeners` 어노테이션을 Entity 클래스에 추가한다. 인자로는 `AuditingEntityListener.class`를 넘긴다. 이 설정을 통해 엔티티에 이벤트가 발생했을 때 정보를 캡처할 수 있다.\n\n```java\n@Entity\n@EntityListeners(AuditingEntityListener.class)\npublic class BaseEntity {\n    // ...\n}\n```\n\n### 생성일시, 수정일시 추적하기\n\n생성일시는 `@CreatedDate`, 수정일시는 `@LastModifiedDate` 어노테이션을 통해 추적할 수 있다. 생성일시의 경우 한 번 생성되면 변경되어선 안 되며, 항상 존재해야 하므로 `nullable = false`, `updatable = false`로 지정한다.\n\n추가적으로 BaseEntity를 다른 Entity들이 상속받아 사용할 수 있도록 `@MappedSuperclass` 어노테이션을 통해 해당 클래스를 Entity가 아닌 SuperClass로 지정했다.\n\n```java\n@MappedSuperclass\n@EntityListeners(AuditingEntityListener.class)\npublic abstract class BaseEntity {\n\n    @CreatedDate\n    @Column(name = \"created_at\", nullable = false, updatable = false)\n    private LocalDateTime createdAt;\n\n    @LastModifiedDate\n    @Column(name = \"updated_at\")\n    private LocalDateTime updatedAt;\n\n    // ...\n}\n```\n\n### 생성한 사람, 수정한 사람 추적하기\n\n생성한 사람은 `@CreatedBy`, 수정한 사람은 `@LastModifiedBy` 어노테이션을 통해 추적할 수 있다. 해당 필드는 생성자, 수정자의 이름으로 채워진다.\n\n```java\n@MappedSuperclass\n@EntityListeners(AuditingEntityListener.class)\npublic abstract class BaseEntity {\n\n    @CreatedBy\n    @Column(name = \"created_by\")\n    private String createdBy;\n\n    @LastModifiedBy\n    @Column(name = \"modified_by\")\n    private String modifiedBy;\n\n    // ...\n}\n```\n\n유저에 대한 정보는 SecurityContext's Authentication 인스턴스로부터 가져온다. 이 값을 커스텀하고 싶다면 `AuditorAware<T>` 인터페이스를 구현해야 한다.\n\n```java\npublic class AuditorAwareImpl implements AuditorAware<String> {\n\n    @Override\n    public String getCurrentAuditor() {\n        // your custom logic\n    }\n}\n```\n\n이렇게 만든 `AuditorAwareImpl`를 사용하려면 Config 클래스에 `AuditorAwareImpl` 인스턴스로 초기화되는 `AuditorAware` 타입의 빈을 설정해주어야 한다. 그리고 `@EnableJpaAuditing` 어노테이션에 `auditorAwareRef=\"auditorProvider\"` 설정을 추가한다.\n\n```java\n@Configuration\n@EnableJpaAuditing(auditorAwareRef=\"auditorProvider\")\npublic class JpaConfig {\n    //...\n\n    @Bean\n    AuditorAware<String> auditorProvider() {\n        return new AuditorAwareImpl();\n    }\n\n    //...\n}\n```\n\n---\n\n### References\n\n[https://www.baeldung.com/database-auditing-jpa](https://www.baeldung.com/database-auditing-jpa)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. Spring Data JPA에서는 Pagination을 위한 두 가지 객체를 제공한다. 바로 Slice와 Page다. Repository 코드를 먼저 보자. 메서드를 보면 파라미터로  객체를 받는다.  객체는 Pagination을 위한 정보를 저장하는 객체다.  인터페이스의 구현체인 의 인스턴스를 생성하여…","fields":{"slug":"/data-jpa-slice-page/"},"frontmatter":{"date":"July 18, 2022","title":"Spring Data JPA의 Slice & Page","tags":["Spring","Data JPA","Slice","Page"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\nSpring Data JPA에서는 Pagination을 위한 두 가지 객체를 제공한다. 바로 Slice와 Page다. Repository 코드를 먼저 보자.\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n    Slice<Member> findSliceBy(final Pageable pageable);\n        Page<Member> findPageBy(final Pageable pageable);\n}\n\n// Controller\n// queryParameter를 통해 page, size를 받는다\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(@RequestParam int page, @RequestParam int size) {\n    // 생략\n}\n\n// Service\nPageRequest pageRequest = PageRequest.of(page, size); // pageRequest 생성\nSlice<Member> slices = memberRepository.findSliceBy(pageRequest); // repository에서 페이지 가져오기\nPage<Member> pages = memberRepository.findPageBy(pageRequest);\n```\n\n메서드를 보면 파라미터로 `Pageable` 객체를 받는다. **`Pageable` 객체는 Pagination을 위한 정보를 저장하는 객체**다. `Pageable` 인터페이스의 구현체인 `PageRequest`의 인스턴스를 생성하여 메서드 인자로 넘겨줄 수 있다.\n\n![PageRequest static constructor](pagerequest.png)\n\n`PageRequest`는 정적 팩토리 메서드 `of`를 사용하여 인스턴스를 생성할 수 있다. `PageRequest`는 인자로 page, size, 필요하다면 sort까지 받을 수 있다.\n\n- `page` : 0부터 시작하는 페이지 인덱스 번호\n- `size` : 한 페이지에 반환할 데이터의 개수\n- `sort` : 정렬 방식\n\npage와 size를 쿼리 파라미터로 받아 `PageRequest`를 생성하여 Repository 메서드에 넘겨주는 것으로 간단하게 Pagination을 구현할 수 있는 것이다.\n\n## Slice VS Page\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n    Slice<Member> findSliceBy(final Pageable pageable);\n        Page<Member> findPageBy(final Pageable pageable);\n}\n```\n\nRepository의 메서드를 보면 반환 값으로 `Slice` 혹은 `Page`를 받을 수 있다. 둘은 어떤 차이가 있을까?\n\n### Slice\n\n![slice](slice.png)\n\n`Slice`는 `Streamable`을 상속받는 인터페이스로 Pagination과 관련된 여러 메서드를 갖고 있다. 대표적인 메서드 몇 가지만 살펴보자.\n\n![slice methods](slice_methods.png)\n\n현재 페이지의 내용을 확인하거나 다음 페이지, 이전 페이지에 대한 정보를 가져올 수 있다. 그렇다면 `Page`는 무엇일까?\n\n### Page\n\n![page](page.png)\n\n`Page`는 `Slice`를 상속한다. 따라서 `Slice`가 가진 모든 메서드를 `Page`도 사용할 수 있다. 다만 `Page`가 다른 점은 **조회 쿼리 이후 전체 데이터 개수를 조회하는 쿼리가 한 번 더 실행된다는 것**이다.\n\nPage가 추가적으로 구현하고 있는 메서드 두 가지만 살펴보자.\n\n![page methods](page_methods.png)\n\nPage의 경우 전체 데이터 개수를 조회하는 쿼리가 추가적으로 실행되므로 Slice와 다르게 전체 데이터 개수나 전체 페이지 수까지 확인할 수 있다.\n\n> 추가적으로..\n\n```java\npublic interface PagingAndSortingRepository<T, ID> extends CrudRepository<T, ID> {\n\n    Iterable<T> findAll(Sort sort);\n\n    Page<T> findAll(Pageable pageable);\n}\n```\n\n`Page`를 반환받을 경우 아래처럼 **`Page`를 반환하는 `findAll` 메서드가 이미 존재**한다. 따로 Repository Interface에 메서드를 추가해주지 않아도 `findAll` 메서드에 `Pageable` 객체를 넘겨주면 Pagination을 사용할 수 있다. (Slice를 반환하려면 메서드를 정의해주어야 한다.)\n\n### Slice VS Page 어떤 걸 사용해야 할까?\n\n`Slice`는 전체 데이터 개수를 조회하지 않고 이전 or 다음 `Slice`가 존재하는지만 확인할 수 있다. 따라서 **`Slice`는 무한 스크롤 등을 구현하는 경우 유용**하다. `Page`에 비해 쿼리가 하나 덜 날아가므로 **데이터 양이 많을수록 `Slice`를 사용하는 것이 성능상 유리**하다.\n\n`Page`는 전체 데이터 개수를 조회하는 쿼리를 한 번 더 실행한다. 따라서 **전체 페이지 개수나 데이터 개수가 필요한 경우 유용**하다.\n\n> 알록에서는 카테고리 조회를 무한 스크롤로 구현하므로 Slice를 사용했다.\n\n## 컨트롤러에서 queryParameter를 Pageable 객체로 받는 방법\n\n앞에선 queryParameter를 통해 page, size를 받아 PageRequest를 만들어 넘겨주는 방법을 설명했다.\n\n```java\n// Controller\n// queryParameter를 통해 page, size를 받는다\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(@RequestParam int page, @RequestParam int size) {\n    // 생략\n}\n```\n\n하지만 이렇게 구현하니까 *“ModelAttribute를 통해 queryParameter를 DTO로 받는 것처럼, queryParameter를 Pageable 객체로 받을 수 없을까?”*하는 의문이 생겼다. 직접 ArgumentResolver를 구현할 뻔했지만 찾아보니 역시 똑똑한 JPA.. Pageable 객체를 인수로 설정하면 어노테이션 없이도 자동으로 객체를 만들어준다. `PageableHandlerMethodArgumentResolver` 가 이미 구현되어 있기 때문이다.\n\n```java\n// Controller\n// Pageable 객체를 바로 받을 수 있다.\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(Pageable pagealbe) {\n    // 생략\n}\n\n// Service\nSlice<Member> slices = memberRepository.findSliceBy(pageable); // PageReqeust를 생성할 필요 없이 바로 객체를 넘겨줄 수 있다.\n```\n\n이렇게 사용하면 PageReqeust를 생성할 필요 없이 바로 객체를 넘겨줄 수 있어 훨씬 편하다. 하지만 문제가 한 가지 발생한다. **프론트에서는 page index를 1부터 계산하는 것과 달리 Pageable은 page index를 0부터 계산한다.** 따라서 page의 인덱스를 1부터 시작하도록 설정할 필요가 있다.\n\n## page 인덱스를 1부터 시작하도록 설정하기\n\n### 방법 1. PageableHandlerMethodArgumentResolverCustomizer 커스터마이징 하기\n\n```java\n@Configuration\npublic class CustomPageableConfiguration {\n    @Bean\n    public PageableHandlerMethodArgumentResolverCustomizer customize() {\n        return p -> p.setOneIndexedParameters(true);\n    }\n}\n```\n\n따로 `CustomPageableConfiguration` 클래스를 만들어 `PageableHandlerMethodArgumentResolverCustomizer`를 커스터마이징 해주는 방법이 있다. 여기서 `setOneIndexedParameters` 메서드를 통해 인덱스를 1부터 시작하게 설정할 수 있다.\n\n### 방법 2. application-properties에 설정 추가하기\n\n```java\nspring.data.web.pageable.one-indexed-parameters=true\n```\n\n### 유의점 1\n\n이렇게 인덱스를 1부터 시작하도록 설정하면 **단순히 Controller에서 받는 Pageable의 page 값이 -1 되어 저장될 뿐이다.** Pageable 객체나 이후에 반환받은 Slice, Page 객체에서는 **page 인덱스가 여전히 0부터 시작**한다. \n\n따라서 getNumber 등의 메서드를 통해 page 번호를 받으면 -1씩 차이가 난다는 사실을 잊어선 안 된다. **현재 페이지 index를 반환하고 싶다면 반드시 -1**을 해줘야 한다. 유의해서 사용하자.\n\n### 유의점 2\n\n**무한 스크롤을 Slice나 Page로 구현하면 데이터가 중복으로 나타날 수 있다.** \n\n예를 들어 생각해보자. 30개의 데이터를 불러왔고 10개의 데이터를 추가로 요청하려 하는데, 그 사이에 10개의 데이터가 앞에 추가되었다. 이 경우 데이터가 10개씩 밀려서 21~30번째 데이터가 중복으로 나타나게 된다.\n달록에서는 아직 이 경우까지 고려해서 페이징을 구현하지 못했다. 좀 더 고민해보아야 할 부분인 것 같다.\n\n### References\n\n[https://tecoble.techcourse.co.kr/post/2021-08-15-pageable/](https://tecoble.techcourse.co.kr/post/2021-08-15-pageable/)\n\n[https://treasurebear.tistory.com/59](https://treasurebear.tistory.com/59)\n\n[https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Slice.html](https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Slice.html)\n\n[https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Page.html](https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Page.html)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 매트가 작성했습니다. 달록에 적절한 패키지 구조 고민하기 우리는 프로젝트를 진행하며 어떠한 패키지 구조를 구성할지 고민하게 된다. 보통 패키지 구조를 나누는 대표적인 방법으로 , 로 나눌 수 있다. 계층별 패키지 구조 계층형 구조는 각 계층을 대표하는 패키지를 기준으로 코드를 구성한다. 계층형 구조의 가장 큰 장점은 해당 프로…","fields":{"slug":"/package-structure/"},"frontmatter":{"date":"July 18, 2022","title":"달록에 적절한 패키지 구조 고민하기","tags":["패키지 구조"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [매트](https://github.com/hyeonic)가 작성했습니다.\n\n## 달록에 적절한 패키지 구조 고민하기\n\n우리는 프로젝트를 진행하며 어떠한 패키지 구조를 구성할지 고민하게 된다. 보통 패키지 구조를 나누는 대표적인 방법으로 `계층별`, `기능별`로 나눌 수 있다.\n\n### 계층별 패키지 구조\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── application\n    │   │               ├── config\n    │   │               ├── domain\n    │   │               ├── dto\n    │   │               ├── exception\n    │   │               ├── presentation\n    │   │               └── DallogApplication.java\n    │   └── resources\n    │       └── application.yml\n```\n\n계층형 구조는 각 계층을 대표하는 패키지를 기준으로 코드를 구성한다. 계층형 구조의 가장 큰 장점은 해당 프로젝트에 대한 이해도가 낮아도 각 계층에 대한 역할을 충분히 숙지하고 있다면 전체적인 구조를 빠르게 파악할 수 있다.\n\n하지만 단점도 존재한다. 하나의 패키지에 모든 클래스들이 모이게 되기 때문에 규모가 커지면 클래스의 개수가 많아져 구분이 어려워진다. 아래는 이전에 계층형 구조를 기반으로 작성한 프로젝트이다.\n\n```json\n└── presentation\n    ├── ChampionController.java\n    ├── CommentController.java\n    ├── PlaylistController.java\n    ├── RankingController.java\n    ├── SearchController.java\n    ├── UserController.java\n    ├── WardController.java\n    └── ...\n```\n\n비교적 적은 코드의 양이지만 규모가 커질수록 애플리케이션에서 presentation 계층에 해당하는 모든 객체가 해당 패키지에 모이게 될 것이다.\n\n### 기능별 패키지 구조\n\n기능별로 패키지를 나눠 구성한다. 기능별 패키지 구조의 장점은 해당 도메인에 관련된 코드들이 응집되어 있다는 점이다. 덕분에 `지역성의 원칙`를 잘 지킬 수 있다고 한다.\n\n> 컴퓨터 과학에서, 참조의 지역성, 또는 지역성의 원칙이란 프로세서가 짧은 시간 동안 동일한 메모리 공간에 반복적으로 접근하는 경향을 의미한다. 참조 지역성엔 두 가지 종류가 있다. 바로 시간적 지역성과 공간적 지역성이다. 시간적 지역성이란 특정 데이터 또는 리소스가 짧은 시간 내에 반복적으로 사용되는 것을 가리킨다. 공간적 지역성이란 상대적으로 가까운 저장 공간에 있는 데이터 요소들이 사용되는 것을 가리킨다.\n\n개발자 역시 복잡하고 거대한 프로젝트의 전체 구조를 모두 인지하는 것은 힘든일이다. 우선 특정 지역의 흐름을 파악할 수 있다면 해당 패키지에 대해서는 마치 캐시에 적재된 데이터에 접근 하듯 빠르게 인지가 가능하다.\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── category\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── schedule\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── global\n    │   │               │   ├── config\n    │   │               │   ├── dto\n    │   │               │   ├── error\n    │   │               │   └── exception\n    │   │               ├── infrastructure\n    │   │               │   └── oauth\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n하지만 각 계층이 기능별로 모여 있기 때문에 프로젝트에 대한 이해도가 낮으면 전체 구조를 파악하는데 오랜 시간이 걸린다.\n\n### 더 나아가기\n\n현재 구조에서는 도메인에 해당하는 다양한 기능들이 패키지 전반적으로 퍼져 있다. 각각의 도메인 기능이 밀집되어 있지 않은 구조를 가지고 있다.\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── domain\n    │   │               │   ├── auth\n    │   │               │   │   ├── application\n    |   |               │   |   ├── domain\n    │   │               │   │   ├── dto\n    │   │               │   │   ├── exception\n    │   │               │   │   └── presentation\n    │   │               │   ├── category\n    │   │               │   │   ├── application\n    |   |               │   |   ├── domain\n    │   │               │   │   ├── dto\n    │   │               │   │   ├── exception\n    │   │               │   │   └── presentation\n    │   │               │   └── schedule\n    │   │               │       ├── application\n    |   |               │       ├── domain\n    │   │               │       ├── dto\n    │   │               │       ├── exception\n    │   │               │       └── presentation\n    │   │               ├── global\n    │   │               │   ├── config\n    │   │               │   ├── dto\n    │   │               │   ├── error\n    │   │               │   └── exception\n    │   │               ├── infrastructure\n    │   │               │   └── oauth\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.properties\n```\n\n### domain\n\n실제 애플리케이션의 핵심이 되는 도메인 로직이 모여 있다. 애플리케이션의 주요 비즈니스 로직이 모여 있기 때문에 `외부와의 의존성을 최소화`해야 한다. 즉 `외부의 변경에 의해 도메인 내부가 변경되는 것을 막아야 한다는 것`을 인지해야 한다.\n\n### global\n\n`global`은 프로젝트 전반에서 사용하는 객체로 구성한다. 공통적으로 사용하는 dto나 error, config에 대한 것들이 모여 있다.\n\n### infrastructure\n\n`infrasturcture`는 외부와의 통신을 담당하는 로직들이 담겨 있다. 이번 프로젝트에서는 OAuth를 활용한 회원 관리를 진행하기 때문에 google의 인증 서버와 통신이 필요해진다. 이 패키지는 우리의 의지와 다르게 외부의 변화에 따라 변경될 여지를 가지고 있다. 즉 변화에 매우 취약한 구조이며 외부 서버에 의존적 이기 때문에 항시 변화에 대응할 수 있도록 대비해야 한다. 이것이 의미하는 바는 결국 `도메인 관련 패키지에서 infrastructure를 직접적으로 의존하는 것`은 도메인 로직을 안전하게 지킬 수 없다는 의미를 내포한다.\n\n## 정리\n\n각각의 방법은 서로 다른 장단점을 가지고 있기 때문에 정답은 없다고 생각한다. 현재 프로젝트의 규모와 요구사항을 고려하여 선택해야 한다. 다만 선택한 패키지 구조에 충분한 근거를 가져야하고, 객체와 패키지 사이의 의존성에 대해 충분히 고민해야 한다.\n\n달록은 현재 기능별 패키지 구조로 진행되고 있다. 모든 팀원들이 처음 부터 기획과 설계에 대한 고민을 진행했기 때문에 프로젝트의 구조에 대해 분석 하는 시간이 불필요했기 때문이다. 하지만 각각의 기능들이 난잡하게 퍼져 있기 때문에 [더 나아가기](#더-나아가기)에서 언급한 것 처럼 좀 더 밀접한 기능들을 모아둘 필요가 있다고 판단한다. 이것은 추후 팀원들과 충분한 논의를 통해 개선해갈 예정이다.\n\n## References.\n\n[Spring Guide - Directory](https://cheese10yun.github.io/spring-guide-directory)<br>\n[지역성의 원칙을 고려한 패키지 구조: 기능별로 나누기](https://ahnheejong.name/articles/package-structure-with-the-principal-of-locality-in-mind)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 나인이 작성했습니다. 🎯 \"무한 스크롤을 구현해보세요!\" 어떻게 구현하실 건가요? 무한 스크롤을 처음 마주했을때 🤔 저는 처음 무한 스크롤을 구현할 때 다음과 같은 방식을 사용했어요. scroll event 사용하기 우테코 레벨1 유튜브 미션 처음 제가 무한 스크롤을 구현했던 방법은 다음과 같습니다. 바로 스크롤 이벤트와 of…","fields":{"slug":"/infinite-scroll/"},"frontmatter":{"date":"July 18, 2022","title":"React에서 무한 스크롤 구현하기","tags":["react"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [나인](https://github.com/jhy979)이 작성했습니다.\n\n🎯 \"무한 스크롤을 구현해보세요!\"\n\n어떻게 구현하실 건가요?\n\n## 무한 스크롤을 처음 마주했을때\n\n🤔 저는 처음 무한 스크롤을 구현할 때 다음과 같은 방식을 사용했어요.\n\n```\n1. scroll 이벤트를 감지한다.\n\n2. 현재 스크롤 영역의 `위치를 계산`한다.\n\n3. 영역 계산을 통해 페이지 아래에 위치하면 API 요청을 진행한다.\n\n4. 받아온 데이터를 추가하여 다시 렌더링한다.\n\n5. 무한 반복\n```\n\n---\n\n## scroll event 사용하기\n\n[우테코 레벨1 유튜브 미션](https://github.dev/jhy979/javascript-youtube-classroom/tree/jhy979-step2)\n\n처음 제가 무한 스크롤을 구현했던 방법은 다음과 같습니다.\n\n바로 스크롤 이벤트와 offset을 이용한 방식이죠!\n\n```js\n  scrollToBottom(callback) {\n    const isScrollBottom =\n      this.$videoList.scrollHeight - this.$videoList.scrollTop <=\n      this.$videoList.offsetHeight + EVENT.SCROLL.OFFSET;\n\n    if (isScrollBottom) {\n      callback(this.$searchInput.value);\n    }\n  }\n```\n\n메서드 네이밍을 통해서도 알 수 있듯이, 화면 하단까지 내려갔을 경우 (offset 정도를 감안하여) 인자로 받은 함수를 실행시켜주었습니다.\n\n아 물론, 스크롤 이벤트는 워낙 많이 발생하기 때문에 throttle을 걸어주었습니다. (이건 필수죠)\n\n😢 하지만, `documentElement.scrollTop`, `documentElement.scrollHeight`, `documentElement.offsetHeight`는 리플로우(Reflow)가 발생합니다.\n\n확실히 비효율적이겠죠!\n\n---\n\n## IntersectionObserver 사용하기\n\n달록에서는 무한 스크롤을 구현할 때 [Intersection Observer](https://developer.mozilla.org/ko/docs/Web/API/Intersection_Observer_API)를 사용했습니다.\n\n> Intersection Observer는 쉽게 말해 지정한 대상이 화면에 보이는지 감시하고 판단하는 도구입니다.\n\n브라우저 Viewport와 Target으로 설정한 요소의 교차점을 관찰하여 그 Target이 Viewport에 포함되는지 구별하는 기능을 제공합니다.\n\n<img src=\"https://velog.velcdn.com/images/jhy979/post/19500233-65fc-4ba9-b421-81516700c00b/image.png\" />\n\n### useIntersect 커스텀훅\n\n> 가장 먼저 useIntersect 라는 커스텀훅을 제작했습니다.\n\n이 커스텀훅은 `인자로 intersect시 실행할 함수`를 받고 `ref를 제공`하여 관찰할 대상을 지정할 수 있습니다.\n\n```ts\ntype IntersectHandler = (\n  entry: IntersectionObserverEntry,\n  observer: IntersectionObserver\n) => void;\n\n// 인자로 onIntersect와 options를 받습니다.\n// onIntersect는 intersect 발생 시 실행하고 싶은 함수입니다.\nfunction useIntersect(\n  onIntersect: IntersectHandler,\n  options?: IntersectionObserverInit\n) {\n  // 관찰하고 싶은 친구를 잡기 위해 ref를 만들어주세요.\n  const ref = useRef<HTMLDivElement>(null);\n\n  // intersect 시 실행할 함수를 만들어줍시다.\n  const callback = useCallback(\n    (entries: IntersectionObserverEntry[], observer: IntersectionObserver) => {\n      entries.forEach((entry) => {\n        if (entry.isIntersecting) {\n          onIntersect(entry, observer);\n        }\n      });\n    },\n    [onIntersect]\n  );\n\n  // 🔨 옵저버에게 일을 시켜봅시다.\n  useEffect(() => {\n    // 우리가 관찰하고 싶은 친구가 없으면 그냥 return 해버려요.\n    if (!ref.current) {\n      return;\n    }\n\n    // 관찰할 대상이 있으면 옵저버 데꼬 와야죠!\n    const observer = new IntersectionObserver(callback, options);\n\n    // 이 옵저버한테 감시를 시킵시다.\n    observer.observe(ref.current);\n\n    // 할 일 끝나면 고생한 옵저버도 쉬게 해줍시다!\n    return () => {\n      observer.disconnect();\n    };\n  }, [ref, options, callback]);\n\n  return ref;\n}\n\nexport default useIntersect;\n```\n\n### 실제 사용\n\nuseIntersect 커스텀훅을 잘 만들었으니 이제 이 커스텀훅을 무한 스크롤에 사용해 봅시다.\n\n![](https://velog.velcdn.com/images/jhy979/post/4643727c-852d-4f23-ab4f-44ce79e2e3b2/image.gif)\n\n다음은 카테고리 목록을 계속 불러와 리스트를 보여주는 컴포넌트입니다.\n\n```tsx\nfunction CategoryList({\n  categoryList,\n  getMoreCategories,\n  hasNextPage,\n}: CategoryListProps) {\n  // useIntersect 커스텀훅의 인자로 (교차 시) 실행할 함수를 넣어줍시다.\n  const ref = useIntersect(() => {\n    hasNextPage && getMoreCategories();\n  });\n\n  return (\n    <div css={categoryTable}>\n      <div css={categoryTableHeader}>\n        <span> 생성 날짜 </span>\n        <span> 카테고리 이름 </span>\n      </div>\n      {categoryList.map((category) => (\n        <CategoryItem key={category.id} category={category} />\n      ))}\n      // 페이지 하단까지 내리면 이 친구가 등장하여 옵저버에게 감지될 거예요.\n      <div ref={ref} css={intersectTarget}></div>\n    </div>\n  );\n}\n```\n\n💪 무한 스크롤함에 따라 props로 받아오는 categoryList가 길어지게 될텐데요, 다행히 React에서는 key값으로 변경 여부를 확인하기 때문에 새롭게 추가된 리스트들만 리렌더링해주었습니다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 매트가 작성했습니다. Git-flow git 브랜치 전략 중 하나이다, 이것은 어떠한 기능을 나타내는 것이 아니라 방법론이다. 각각의 프로젝트와 개발 환경에 따라 알맞게 수정하여 사용해야 한다. 이 게시글은 git을 알고 사용해 본 경험이 있다는 것을 전제로 작성하였다. 또한 직접 프로젝트에 적용하고 연습하고 있기 때문에 정답…","fields":{"slug":"/git-branch-strategy/"},"frontmatter":{"date":"July 12, 2022","title":"달록팀의 git 브랜치 전략을 소개합니다.","tags":["git","git-flow"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [매트](https://github.com/hyeonic)가 작성했습니다.\n\n## Git-flow\n\ngit 브랜치 전략 중 하나이다, 이것은 어떠한 기능을 나타내는 것이 아니라 방법론이다. 각각의 프로젝트와 개발 환경에 따라 알맞게 수정하여 사용해야 한다.\n\n이 게시글은 git을 알고 사용해 본 경험이 있다는 것을 전제로 작성하였다. 또한 직접 프로젝트에 적용하고 연습하고 있기 때문에 정답이 될 수 없고, 지속적으로 개선할 예정이다.\n\n## Git Repository\n\n프로젝트에 적용하기 앞서 어떠한 형태로 Git Repository가 구성되는지 살펴보았다.\n\n![](./git-repository.png)\n\n### Upstream Remote Repository\n\n개발자가 공유하는 저장소로 최신 소스코드가 저장되어 있는 원격 저장소이다.\n\n#### 적용하기\n\n이러한 Remote Repository 생성을 위하여 github에 New organization을 사용히였다.\n\n![](./new-organization.png)\n\n다양한 기능을 제공하는 Team과 Enterprice는 월마다 일정 금액을 사용해야 한다. 하지만 간단한 프로젝트 진행을 위해 생성하였기 때문에 Free만 사용하여도 충분한 실습과 프로젝트를 진행할 수 있다.\n\n![](./fare.png)\n\norganization을 생성하게 되면 소속된 repository를 생성할 수 있다. 이것을 `Upstream Remote Repository`로 적용한다.\n\n### A's, B's, C's Origin Remote Repository\n\n`Upstream Repository`를 Fork한 원격 개인 저장소이다. Upstream Repository를 직접 clone하여 작업하는 것이 아니라 각각의 팀원들이 `Fork`를 하여 원격 저장소를 생성하고 그것을 clone하여 `Local Repository`를 생성하여 작업한다.\n\n이렇게 두 개의 remote repository로 나눈 이유는 Upstream repository의 경우 `팀원이 공유`하고 있는 Repository이기 때문에 다양한 시도를 하기에 큰 위험 부담을 가지고 있다. 각자의 개인 repository에서 `작업을 시도`한 후 적절한 기능 merge 하기 위해 `Pull Request`를 요청한다.\n\n### 운영 방식\n\ngit-flow는 기본적으로 5가지의 브랜치를 사용하여 운영한다.\n\n- `main`: 제품으로 출시될 수 있는 브랜치\n- `develop`: 다음 출시 버전을 개발하는 브랜치\n- `feature`: 기능을 개발하는 브랜치\n- `release`: 이번 출시 버전을 준비하는 브랜치\n- `hotfix`: 출시 버전에서 발생한 버그를 수정하는 브랜치\n\n![](./git-flow-dev.png)\n\n`main`과 `develop` 브랜치이다. 두 브랜치는 항시 운영되어야 하는 브랜치이다. `develop`는 개발을 위한 브랜치이고, `main`은 제품으로 출시될 수 있는 브랜치 이기 때문에 `항시 배포 가능한 상태`이어야 한다.\n\n`main`과 `develop`은 `Upstream remote repository`에서 운영한다.\n\n![](./git-flow-feature.png)\n\n`feature` 브랜치는 단위 기능을 개발하는 브랜치이다. 기능 개발이 완료되면 `develop` 브랜치와 `merge` 된다.\n\n`develop`은 모든 팀원이 `공유`하는 브랜치이다. feature는 각자 맡아 작성한 코드들이 들어 있는 브랜치이다. merge 작업 전에 팀원들 간의 `지속적인 코드 리뷰`가 필요하다.\n\n그렇기 때문에 `Pull Request`를 사용하여 `merge` 작업 전 리뷰어들에게 코드 리뷰를 받고 반영 사항을 수정하여 commit 후 merge 한다. 이 과정은 `협업에서 가장 중요한 부분`이라고 생각된다.\n\n![](./git-flow-release.png)\n\n`release` 브랜치는 배포를 하기 전에 충분한 검증을 위해 생성하는 브랜치이다. 배포 가능한 상태가 되면 `main` 브랜치로 `merge` 작업을 거친다. 또한 `develop`에도 반영사항을 모두 `merge` 시켜야 한다.\n\n![](./git-flow-hotfix.png)\n\n`hotifx` 브랜치는 배포 중 버그가 생겨 긴급하게 수정해야 하는 브랜치이다. 배포 이후에 이루어지는 브랜치이고, 반영 사항을 `main`과 `develop`에 모두 적용 시켜야 한다.\n\n앞서 말했듯이 `main`과 `develop`는 항시 운영되는 브랜치이다. 이 둘을 제외한 나머지 브랜치 들은 제 역할이 마무리 되어 `merge` 작업이 완료되면 브랜치를 삭제하여 정리한다.\n\n### 간단히 적용해보기\n\n`Upstream Remote Repository`를 기반으로 원격 개인 저장소에 `Fork` 해야 한다.\n\n![](./fork.png)\n\nOrganization에 생성한 repository에 Fork를 누르면 손쉽게 할 수 있다. Fork로 생성된 repository를 기반으로 `Local Repository`를 생성해야 한다.\n\n```bash\ngit clone https://github.com/{개인 github 이름}/{repository 이름}.git\n```\n\ngit clone을 사용하여 원격 저장소에 있는 repository를 손쉽게 clone할 수 있다.\n\n```bash\n$ git remote -v\n\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (fetch)\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (push)\n```\n\nclone 받은 local repository를 git remote -v로 확인해보면 원격 저장소가 등록되어 있는 것을 확인 할 수 있다. 매번 최신 코드를 `fetch` 및 `rebase` 하기 위해서는 `Upstream`을 등록해야 한다.\n\n```bash\n$ git remote add upstream https://github.com/{organization 이름}/{repository 이름}.git\n\n$ git remote -v\n\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (fetch)\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (push)\nupstream        https://github.com/{organization 이름}/{repository 이름}.git\nupstream        https://github.com/{organization 이름}/{repository 이름}.git\n```\n\n`git remote add upstream`을 통하여 upstream을 등록한다. 정상적으로 등록 된 것을 확인할 수 있다. 이제 작업할 때 마다 브랜치를 생성하고 최신 코드를 pull 받아야 한다.\n\n우리 팀원은 각각 개발해야 하는 기능을 github issue에 등록한 후 등록 번호를 기반으로 브랜치를 생성하기로 하였다.\n\n우선 간단한 예시를 위하여 이슈를 등록한다.\n\n![](./issue.png)\n\n3번 번호가 부여된 이슈라고 가정한다. 해당 번호를 기반으로 `local repository`에서 feature 브랜치를 생성한다.\n\n```bash\n$ git branch feature/3-init-setting\n$ git checkout feature/3-init-setting\n```\n\n이제 Upstream에 있는 remote repository에서 최신 소스코드를 받아 와야 한다.\n\n```bash\n$ git fetch upstream\n$ git rebase upstream/develop\n```\n\ngit pull을 사용하여 등록한 upstream develop에서 commit 기록을 병합한다. 이제 신나게 작업을 진행하고 자신의 원격 저장소인 `Origin remote repository`에 push한다.\n\n```bash\n$ git push origin feature/3-init-setting\n```\n\n그렇게 github repository를 살펴보면 `변경을 감지`하고 pull request를 생성할 것인지에 대한 탭을 확인할 수 있다.\n\n![](./upstream-repository.png)\n\n이제 fork한 개인 원격 저장소를 살펴보면 새롭게 작성한 브랜치를 감지하고 pull request 작성을 위한 버튼이 생성된다.\n\n![](./pull-request.png)\n\n`feature/3-init-setting` 브랜치를 develop에 merge하기 위한 pull request를 진행하는 예시이다. 작성한 코드를 리뷰해줄 팀원들을 선택하고, commit한 코드의 내용을 간단히 요약하여 작성한다. 이제 생성한 PR을 기반으로 `코드리뷰`를 진행한다. 변경 사항이 적용되면 develop에 반영하기 위해 merge한다.\n\n### 달록에 맞게 수정하기\n\ngit-flow는 빠르게 급변하는 웹 서비스에는 맞지 않은 git 브랜치 전략이다. 관리해야 할 브랜치가 늘어나기 때문에 개발자들이 신경써야 할 포인트가 늘어난다.\n\n빈번한 배포로 인해 급작스러운 이슈가 발생할 수 있다. 즉 예상치 못한 롤백이 자주일어날 수 있다. 또한 웹 서비스의 특성상 다양한 릴리즈 버전을 유지할 필요가 없다. 이러한 특성들로 인해 웹 서비스에는 다소 보수적인 git-flow 전략은 맞지 않을 수 있다.\n\n그럼에도 우리 달록팀은 git-flow를 선택했다. 우리는 실제 운영할 수 있는 서비스를 개발하며 다양한 경험을 습득해야 한다. 또한 대부분의 팀원들이 git에 익숙하지 않았으며 다양한 시도를 통해 빠르게 학습해야 한다.\n\n대신 git-flow를 정석적으로 사용하지 않고 필요한 부분을 수정하여 반영하려 한다. 현재 수준에서 `develop`에서 대부분의 빌드를 진행하기 때문에 `release` 브랜치의 필요성이 다소 옅어졌다. 결국 `release`를 제외한 `main`, `develop`, `feature`, `hotfix`만 사용하기로 결정하였다.\n\n### 달록이 집중한 것\n\n달록의 팀 각 구성원들은 맡은 이슈를 기반으로 브랜치를 생성한 뒤 작업을 진행할 것이다. 결국 다수의 브랜치가 아래와 같이 병렬적으로 커밋이 쌓이게 된다.\n\n![](./force-push.png)\n\n만약 팀원 중 한명이 작업을 끝내서 PR이 merge된 상황이라고 가정하자. develop 브랜치의 커밋 베이스는 변경됬으며 이전에 작업을 진행하던 브랜치들은 시작점이 뒤로 밀려나게 된다.\n\n여러 사람이 하나의 저장소를 기반으로 작업을 진행하기 때문에 함께 사용하는 공간의 코드들은 충돌을 야기할 가능성이 크다. 즉 지속적인 `fetch` + `rebase`를 통해 사전에 충돌에 대비하며 항상 `develop` 브랜치와 커밋 싱크를 맞춘다.\n\n정리하면 위 그림과 같이 항시 develop 브랜치의 끝 단에서 시작해야 한다. 이러한 방식은 코드의 충돌을 최소화할 수 있으며 순차적인 git 커밋 목록을 기반으로 쉽게 기능이 추가된 것을 확인할 수 있다.\n\n## 정리\n\n지금까지 간단히 `git-flow의 흐름`과 이것을 기반으로 `달록에 적용한 과정`들을 알아보았다. git-flow는 언급한 것 처럼 부가적인 브랜치로 인해 `관리에 대한 부담감`을 느낄 수 있다. 하지만 `upstream`과 `origin`을 분리한 환경은 좀 더 도전적인 과제들을 적용하기에 매우 좋은 환경을 구성해준다. 또한 `pull request`를 통한 코드 리뷰를 통해 보다 더 양질의 애플리케이션 개발에 힘쓸 수 있다.\n\n## References.\n\n[git flow; 환상과 현실 그 사이에 서비스](https://vallista.kr/git-flow;-%ED%99%98%EC%83%81%EA%B3%BC-%ED%98%84%EC%8B%A4-%EA%B7%B8-%EC%82%AC%EC%9D%B4%EC%97%90-%EC%84%9C%EB%B9%84%EC%8A%A4/)<br>\n[우린 Git-flow를 사용하고 있어요](https://woowabros.github.io/experience/2017/10/30/baemin-mobile-git-branch-strategy.html)<br>\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 배경 우테코 레벨3 달록 팀에서 메소드의 파라미터에는 반드시  키워드를 붙이도록 컨벤션을 정했습니다. 이유는 무엇일까요? 일반적으로 가변적인 변수는 프로그램의 흐름을 예측하기 어렵게 만듭니다. 따라서 변수를 가변적으로 만드는 것이 중요한데, 자바에서는 변수의 재할당을 막기 위해  키워드를 사용합니다. 물론…","fields":{"slug":"/intellij-final-keyword/"},"frontmatter":{"date":"July 12, 2022","title":"IntelliJ에서 메소드 추출한 메소드의 파라미터에 final 키워드 자동 추가하기","tags":["intellij"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n## 배경\n\n우테코 레벨3 달록 팀에서 메소드의 파라미터에는 반드시 `final` 키워드를 붙이도록 컨벤션을 정했습니다. 이유는 무엇일까요? 일반적으로 가변적인 변수는 프로그램의 흐름을 예측하기 어렵게 만듭니다. 따라서 변수를 가변적으로 만드는 것이 중요한데, 자바에서는 변수의 재할당을 막기 위해 `final` 키워드를 사용합니다. 물론 `final` 키워드 하나만으로 완전한 불변을 보장하도록 만들수는 없지만, 어느정도 예측 가능한 코드를 만드는데에는 도움이 됩니다.\n\n이는 메소드의 파라미터에도 적용됩니다. 아래의 코드는 `Memo` 객체를 생성하기 위한 생성자입니다. `value` 라는 String 값을 전달받아 객체 필드에 할당합니다.\n\n```java\npublic Memo(String value) {\n    validateLength(value);\n\n    value = \"hello\"; // 예상치 못한 코드\n\n    this.value = value;\n}\n```\n\n하지만 위처럼 예상치 못한 코드가 추가되면 어떻게 될까요? `value` 필드에는 개발자가 의도하지 못한 값이 할당될 것 입니다.\n\n```java\npublic Memo(final String value) {\n    validateLength(value);\n\n    value = 1; // error: final parameter value may not be assigned\n\n    this.value = value;\n}\n```\n\n이를 보완하기 위해서 위처럼 메소드 파라미터에 `final` 키워드를 붙이면, 재할당 시 컴파일 에러가 발생하여 예상치 못한 동작을 사전에 방지할 수 있을 것 입니다.\n\n## IntelliJ 설정하기\n\n하지만, 저희는 아직 메소드 파라미터에 `final` 키워드를 붙이는 습관이 들어있지 않았습니다. 따라서 IDE의 도움이 필요한데요, 다행히도 IntelliJ에서 메소드 추출 리팩토링을 할 때 생성되는 메소드 파라미터에 자동으로 `final` 키워드를 붙여주는 옵션을 발견하였습니다.\n\n![IntelliJ 설정 화면](./intellij.png)\n\n맥 기준으로 Preferences → Editor → Code Style → Java 페이지에서 Code Generation 탭을 클릭합니다. 해당 탭의 하단에 ‘Final Modifier’ 에서 ‘Make generated parameters final’ 을 체크해줍니다. 위와 같이 옵션을 변경하면 메소드 추출 시 파라미터에 자동으로 `final` 키워드가 생성되는 모습을 확인할 수 있습니다 😊\n"},{"excerpt":"이 글은 우테코 달록팀 크루 리버가 작성했습니다. JPA 등장배경 1990년대 인터넷이 보급되면서 온라인 비지니스가 활성화 되었다.\n자연스럽게, 온라인 비지니스에서 DB에 데이터를 저장하고 가져올때 사용할 Connection Connector에 대한 니즈가 높아졌다.\n그래서 각 언어들에서 DB Connection을 지원하는 API 기술들이 등장하였다. 이…","fields":{"slug":"/appearance-background-of-jpa/"},"frontmatter":{"date":"July 07, 2022","title":"JPA 등장배경","tags":["JPA"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [리버](https://github.com/gudonghee2000)가 작성했습니다.\n\n## JPA 등장배경\n\n1990년대 인터넷이 보급되면서 온라인 비지니스가 활성화 되었다.\n자연스럽게, 온라인 비지니스에서 DB에 데이터를 저장하고 가져올때 사용할 Connection Connector에 대한 니즈가 높아졌다.\n그래서 각 언어들에서 DB Connection을 지원하는 API 기술들이 등장하였다. 이후에 Spring에서는 DB Connection을 좀 더 쉽게 관리하는 Spring JDBC API를 만들고 지원하였다. (이외에도 Query문을 XML파일을 통해 관리하게끔 도와주는 Mybatis도 등장하였음)\n하지만, 여전히 쿼리문을 개발자가 직접 작성해야하는 등 다양한 문제를 가지고 있었다.\n그래서 JAVA 진영에서는 개발자가 쿼리문을 직접 작성하지 않아도 프레임워크 내부에서 지원해주는 ORM(Object Relational Model)기술인 JPA가 등장하였다.\n그렇다면, JPA 이전에 개발자들이 직접 쿼리문을 작성하던 SQL 중심적인 개발의 단점은 무엇이 있을까?\n아래에서 살펴보자.\n\n## SQL 중심적인 개발의 단점\n\n#### 1. 쿼리문 무한 반복과 지루한 코딩\n\nJDBC API는 쿼리문을 개발자들이 직접 작성 해야한다.\n그래서 개발자들은 쿼리문을 작성하는 지루한 작업을 개발 과정에서 무한반복해야한다.\n\n#### 2. 객체의 필드가 추가되면 모든 쿼리문을 수정해야한다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/488a7899-d55b-4447-aafc-bada8d840192/image.jpg)\n위 그림과 같이 SQL 중심적인 개발에서는 객체의 필드가 변경되면 해당하는 모든 쿼리문을 찾아 개발자가 수정해야한다.\n\n#### 3. 객체와 관계형 DB의 패러다임의 불일치\n\n객체라고 하면 떠오르는 키워드는 무엇이 있을까?\n캡슐화, 협력, 의존, 상속, 참조 등의 기술이 있다. 그런데, DB에서는 이러한 기술들이 없다.\n적용되는 기술들의 패러다임 불일치로 인해 개발자들은 SQL 지향적인 개발을 할 수 밖에 없다.\n아래에서 자세히 살펴보자.\n\n## 객체와 관계형 DB의 패러다임 차이\n\n객체와 관계형 DB는 연관관계를 통해 작업을 수행한다는 공통점을 가진다.\n하지만 연관관계를 맺는 패러다임의 차이를 가진다.\n\n객체는 상속, 참조를 통해 연관관계를 맺는다.\n반면 관계형 DB는 PK, FK를 통해 연관관계를 맺는다.\n이때, 연관관계를 맺는 방식의 차이로 발생하는 문제점을 코드와 함께 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Team team;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 `Crew` 객체가 `Team` 객체를 필드로 가지고 참조한다고 하자.\n객체지향적인 관점에서, `Crew`와 `Team`의 관계를 위와 같이 표현하는것은 자연스럽다.\n\n하지만, DB에서는 `Crew`가 `Team`을 참조한다는 개념이 없다.\n그래서 위 객체들을 가지고 DB의 `Crew`테이블과 `Team`테이블의 관계를 맺을때, 아래와 같이 `PK` 값인 id를 `FK`로 가지도록 구현 하여야한다.\n![ERD](./erd.png)\n\n연관관계에 대해서 객체의 구조와 DB의 구조가 달라진다는 것이다.\n\n그렇다면 객체지향적인 연관관계를 가진 객체들을 DB에 저장 할 때,\nDB의 연관관계로 변경하는 것이 왜 문제가 될까?\n\n## 객체와 RDB의 연관관계 차이가 가져오는 문제\n\n위에서 봤던 `Crew`와 `Team`의 객체 모델링을 다시한번 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Team team;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 모델링된 `Crew`와 `Team`을 DB에 저장한다고 한다면 다음의 과정이 필요하다.\nDB에 접근하고자 하는 Dao 객체는 `Crew`객체를 분해하고 각자 `Crew` 테이블과 `Team` 테이블에 대한 쿼리를 작성해야한다. 단순히 `Crew`의 객체정보를 저장하는데 3가지 과정을 거쳐야한다.\n\n이러한 복잡한 과정을 피하는 방법은 없을까?\n아래 코드를 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Long team_id;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 DB 테이블 구조에 맞추어 `Crew`와 `Team` 객체를 설계하는 방법이있다.\n이러한 객체 모델링은 Dao 객체를 통해 데이터를 DB에 저장 할 때, `Crew` 객체를 분해하는 과정을 삭제 할 수 있다.\n객체가 DB 구조에 맞추어 설계되어 있기 때문이다.\n\n**하지만, 객체 모델링을 할 때 객체가 서로 참조하는 객체지향적인 개발이 아닌\nDB 테이블구조에 맞추어 개발하는 SQL 중심적인 개발을 하게 된다는 문제를 가진다.**\n\n객체와 관계형 DB의 연관관계의 패러다임 차이는 객체를 객체답지 못하게 만든다는 것이다.\n그렇다면, 패러다임의 차이를 해결하는 방법은 없을까?\n\n## JPA\n\n객체와 관계형 DB의 패러다임의 차이로 인해 우리는 객체지향적인 프로그래밍을 하지못하고 DB에 종속적인 개발을 하게된다.\n이러한 문제를 해결하기 위해 JAVA진영에서는 JPA를 제공한다.\n\nJPA를 통해 개발자는 더이상 쿼리문을 반복적으로 작성하거나 유지보수하는 것을 신경쓰지 않아도 된다.\n왜냐하면 JPA가 쿼리문을 작성해주기 때문이다.\n그리고 SQL 중심적인 개발에서 벗어나 객체지향적인 개발을 할 수 있게 된다.\n왜냐하면 패러다임의 불일치를 JPA가 내부적으로 맵핑해주기 때문이다.\n\n다음 포스팅에서는 JPA의 작동 메커니즘을 자세히 살펴보자.\n"}]}},"pageContext":{}},"staticQueryHashes":[]}